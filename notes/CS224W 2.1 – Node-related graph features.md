#cs224w #gnn-reading-group #graphic-content 
### Preface for section 2
This section of the course discusses the pre-GNN paradigm. If we were looking to use a more off-the-shelf approach, like an MLP, we would need to find out how to best encode a graph as a set of input features.
## Node-level prediction
The task of node-level prediction under this more "traditional" paradigm reduces to the problem of finding $$f(\mathrm{node~\nu}) \rightarrow \mathrm{result~about~node~\nu}.$$
Here, $f$ is likely going to be some learned function (e.g. a feed-forward neural network), and knows nothing about the graph itself.

To make a statement about a node $\nu$, we begin by assuming that nodes already have their feature vectors associated with them, which we've discussed previously (and used notation $\mathbf{x}_\nu$ for). These will comprise some kind of domain information (e.g. in a social network, each node would be a person, and carry relevant metadata for that person, like biometrics, date of joining... stuff like this). 

Along with these feature vectors, we now are required to encode the graph itself; there is no way for $f$ to know about the graph structure unless we tell it otherwise! For each node, we will then need to include some other information about its relative importance in the graph structure, since we've put the actual graph to one side for now. We'll have to craftily hand-design these features.

## Graph-related node features
Our goal: come up with some numbers that **characterize the structure and position of a particular node in the graph**. Why is this useful? Consider the following picture:![[Pasted image 20240321130556.png]]
Look at the graphs on the left and right. Can you tell the rule that makes the nodes colored red or green?

The answer is that red nodes have one connection, and green nodes have multiple. If the arrow labelled "machine learning" is to do its job correctly, it should definitely understand these notions of connectivity to exploit any geometric relations between nodes and edges.

### Node degree
A first notion to look at here is **node degree** $k_\nu$, defined as the number of connections a node has. Here's an illustration at nodes with different degrees: ![[Pasted image 20240321130940.png]]
While this _is_ information about the graph, it's also misleading: we can have nodes with the same degree, but they might be very different in terms of their function in the graph! One could imagine a graph of organs in the body â€” perhaps my foot and my heart have a similar number of veins in them, but I know which one I'd rather assign higher importance. We are then left wanting for some different ways to define node importance.

## Node centrality
There are a few measures of node **centrality** that we'll discuss, which is defined as a relative importance measure of a node, usually corresponding to some notion of how "central" the node is in the network. Think of something like a key transit hub in a country, or the friend that's the "center" of a social network.

### Betweenness centrality
For a traffic graph, a node could be considered important if it's a convenient way to get between many places quickly. For me, Liverpool Street is an example of this, since it always allows me to switch to the central/hammersmith & city/circle/metropolitan lines. Framing this using graph language would be to say that it lines on the **shortest paths between many other node pairs**. ![[Screenshot 2024-03-21 at 13.41.16.png]]
### Closeness centrality
![[Pasted image 20240321134318.png]]

### Eigenvector centrality
Whilst sounding a bit complicated, eigenvector centrality poses one thing: "I am as important as the sum of how important my friends are". Let's write that down in an equation, where we'll use $c_\nu$ as our measure of importance: $$c_v = \sum_{u \in \mathcal{N}(v)} c_u,$$with notation of $\mathcal{N}(v)$ for the 1-hop neighborhood of $v$. If we add some arbitrary postitive normalization $\lambda$ to this, we can actually take advantage of the adjacency matrix $\mathbf{A}$ to re-write this sum: $$c_v = \frac{1}{\lambda}\sum_{u \neq v} A_{uv} c_u ~ \Rightarrow ~ \mathbf{Ac} = \lambda\mathbf{c}.$$
Casting things in this form allows us to extract a precise definition of $c_v$ from taking the solutions to this eigenvalue equation. Specifically, due to the Perron-Frobenius theorem, the largest eigenvalue of this equation $\lambda_{max}$ is both *positive* and *unique*. It is the eigenvector associated with this ($\mathbf{c}_{max}$, to give it a name) that is used as the centrality measure.


## Clustering coefficient
How connected are my friends? That's another measure that could influence your relative importance in a graph.
![[Pasted image 20240321134456.png]]
## Graphlets
A term called **ego-network** can be defined as the graph formed by a given node and its 1-hop neighbors. ![[Pasted image 20240321135014.png]]
![[Pasted image 20240321134838.png]] ![[Pasted image 20240321135138.png]]