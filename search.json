[
  {
    "objectID": "sessions.html",
    "href": "sessions.html",
    "title": "Sessions",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nPresenter\n\n\nDescription\n\n\nType\n\n\n\n\n\n\n \n\n\nGraphs and Sets (Geometric Deep Learning)\n\n\n \n\n\n\n\n\nTheory\n\n\n\n\nJul 4, 2024\n\n\nPredicting Dynamic Embedding Trajectory in Temporal Interaction Networks\n\n\nBoyko\n\n\nIntroduce the JODIE model for modelling sequential user-item interactions in recommender systems.\n\n\nPaper\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "sessions/jodie.html",
    "href": "sessions/jodie.html",
    "title": "Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks",
    "section": "",
    "text": "Recommender systems are an important application of ML on graphs, and a lot of thought has been put into the development of technology to do recommendation well. A central problem in the sphere is dealing with the modeling of sequential interactions between users and items. The items could be products in e-commerce, but also pins on Pinterest, tweets on Twitter, or course contents in Coursera. The paper (Kumar, Zhang, and Leskovec 2019) introduces JODIE, a popular method in the field which has turned into a common benchmark. Some of the challenges it addresses are:\n1. User behaviour changes in time.\n2. Data is fed in as a sequence of interactions.\n3. Interactions have to be processed in sequence, so how do you parallelise?\nConsider the example of a social media. These platforms want to be able to successfully predict what content the users will interact with, so that they can recommend similar content and maximise engagement. In the JODIE model, each user has a stationary property \\(\\overline{\\mathbf{u}}\\), and a time-evolving property \\(\\mathbf{u}(t)\\). This is easy to intuit since people tend to have long-term stable preferences, but their daily and weekly interests can vary. Even cat videos can get boring if that’s all you see, but if you are a person who likes animals, then there are other similar recommendations that will catch your interest. The same argument is applied to the items, and we denote \\(\\overline{\\mathbf{i}}\\), \\(\\mathbf{i}(t)\\) as the stationary and time-evolving representations respectively.\n\n\n\n\n\n\n\n\n\nEmbedding trajectory visualisation, taken from the paper. JODIE is trained to project the future position of an entity, hence taking into account the time-evolving properties of the users and items.\n\n\n\n\n\n\n\nDiagram illustrating the green user projection specifically. Note that the projection for larger timesteps lies further away. The solid lines illustrates the next true embedding, once the model observes the user interacting with an item.\n\n\n\n\n\nThe observed data is the sequence of interactions \\(S_r = (u_r, i_r, t_r, \\mathbf{f}_r)\\), where \\(u_r\\), \\(i_r\\) are the user and item interacting, \\(t_r\\) is the timestamp, and \\(\\mathbf{f}_r\\) is a feature vector associated with the interaction. Once a new interaction is observed, the time-evolving embeddings of the participating entities are updated using a recurrent network:\n\\[\\begin{equation}\n    \\mathbf{u}(t) = \\sigma(W_1^{u}\\mathbf{u}(t^{-}) + W_2^{u}\\mathbf{i}(t^{-}) + W_3^{u}\\mathbf{f} + W_4^{u}\\Delta_u)\n\\end{equation}\\]\n\\[\\begin{equation}\n    \\mathbf{i}(t) = \\sigma(W_1^{i}\\mathbf{i}(t^{-}) + W_2^{i}\\mathbf{u}(t^{-}) + W_3^{i}\\mathbf{f} + W_4^{i}\\Delta_i)\n\\end{equation}\\]\nWhere the interaction occurs at time \\(t\\), \\(\\mathbf{u}(t^{-})\\) and \\(\\mathbf{i}(t^{-})\\) represent the latest embeddings before the current interaction, and \\(W\\) are learnable parameters. The \\(\\Delta\\) represent the elapsed time from the last interaction of the entity. Note that the RNNs are trained to predict the embedding of the item at \\(u\\)’s next interaction (will come back to this). This step is referred to as the update operation.\nThere are two more steps to predicting which item the user will interact with. The first step involves predicting the user embedding trajectory into the future, and is called project operation. The projected embedding \\(\\hat{\\mathbf{u}}(t + \\Delta)\\) is used in the downstream task. In the JODIE model, the user trajectory evolves continuously in time, so for an arbitrarily small \\(\\Delta\\), the projected embedding is arbitrarily close to the starting point. With time, the projection drifts farther away. The \\(*\\) operator is a Hamadard product, and \\(W_p\\) converts \\(\\Delta\\) to a “time-context vector”. They also call \\(W_p\\Delta\\) a temporal attention vector. The project operation aims to predict the user’s preferences in the future, and it is also used to predict the most likely item the user will interact with.\n\\[\\begin{equation}\n    \\hat{\\mathbf{u}}(t + \\Delta) = (1 + W_p\\Delta) * \\mathbf{u}(t)\n\\end{equation}\\]\n\n\n\nDiagram of the JODIE model: After an interaction (u, i, t, ) between user u and item i, the dynamic embeddings of u and i are updated in the update operation with \\(\\texttt{RNN}_U\\) and \\(\\texttt{RNN}_I\\) , respectively. The projection operation predicts the user embedding at a future time \\(t+\\Delta\\)\n\n\nFinally, let user \\(u\\) interact with item \\(i\\) at time \\(t\\), and item \\(j\\) at time \\(t + \\Delta\\). At time \\(t\\) can we predict which item \\(u\\) will interact with at \\(t + \\Delta\\)? A crucial design decision if that JODIE outputs an item embedding vector \\(\\tilde{\\mathbf{j}}(t + \\Delta)\\), instead of an interaction probability between all items. The motivation is that the model has to work on huge datasets, and pairwise probability does not scale well.\n\\[\\begin{equation}\n    \\tilde{\\mathbf{j}}(t + \\Delta) = W_1 \\hat{\\mathbf{u}}(t + \\Delta) + W_2 \\overline{\\mathbf{u}} + W_3\\mathbf{i}(t + \\Delta^{-}) + W_4\\overline{\\mathbf{i}} + \\mathbf{B}\n\\end{equation}\\]\nOutputting \\(\\tilde{\\mathbf{j}}\\) means Locality Sensitive Hashing (LSH) techniques can be used, and the nearest neighbour can be retreived in near-constant time. Note that to compute \\(\\tilde{\\mathbf{j}}\\), the embedding \\(\\mathbf{i}(t + \\Delta^{-})\\) is used, which is the latest embedding of the last item that the user \\(u\\) interacted with. The rationale of including \\(\\mathbf{i}(t + \\Delta^{-})\\) is that the last item’s embedding might have been updated due to interacting with other users, and thus contains new information. Another point of interest in recommendation is that users tend to interact with the same items repeatedly, thus including this information adds useful information.\nThe training objective minimises the L2 loss between the \\(\\tilde{\\mathbf{j}}\\) and the item the user really interacts with at \\(t + \\Delta\\).\n\\[\\begin{equation}\n    \\texttt{L} = \\sum_{S_r \\in S} \\left\\lVert\n    \\tilde{\\mathbf{j}}(t) - [\\overline{\\mathbf{i}}, \\mathbf{i}(t^{-})] \\right\\rVert^2 + \\\\\n    \\lambda_U\\left\\lVert\\mathbf{u}(t) - \\mathbf{u}(t^{-})\\right\\rVert^2 + \\lambda_I\\left\\lVert\\mathbf{i}(t) - \\mathbf{i}(t^{-})\\right\\rVert^2\n\\end{equation}\\]\nIn the above equation, \\([\\cdot, \\cdot]\\) is the concatenation operator, and \\(\\lambda_{U}\\), \\(\\lambda_{I}\\) weigh the regularisation. The regularisation ensures that the updated embeddings do not vary too greatly from the initial embeddings.\nHow do we parallelise the training? One extreme is to process each interaction in sequence of occurrence, but this would not scale to massive interaction graphs. Another extreme would be to randomly subsample a set of interactions and process them in parallel, but this is not viable, because the model is recurrent, and the sequence of processed batches must respect the temporal order of interactions. This is because the user and item embeddings also act as the hidden states of the RNN, and they persist between batches, so you don’t want to update a user’s embedding with interactions that are out of order - the model would be unable to learn the temporal dependency. A nice property of the model is that two interactions that occur between two distinct user-item pairs could be ran together in parallel, as they would not influence each other’s embeddings. Keeping this in mind, and respecting the temporal order of interactions, the paper introduces the t-batch algorithm. This algorithm assigns all \\(|\\mathcal{I}|\\) interactions to \\(B_k\\) batches, where \\(k \\in [1, |\\mathcal{I}|]\\). Each batch contains user-item interactions such that each user and item appear only once. The interactions contained in two batches will also be in order.\n\n\n\nAn attempt to visualise tbatch. The board represents the interaction matrix between users and items. A pebble on the board represents that an interaction has occurred. The pebbles are coloured by batch, such that within a batch the user and item does not repeat. For any pair of batches, if a user or item interacts in both of them, their interactions must be in the same temporal order as the batch sequence.\n\n\nAt the time when the paper was published, JODIE achieved SOTA performance. Nowadays it acts as a strong benchmark. We summarised how the model deals with the time evolving user behaviour, and how it is trained. The model should work well in an online production setting, as it can constantly update user and item state as new observations come in. Maybe that would reduce the need for re-training, and might be a good way to address the changing trends in online media, without worrying as much about distribution shift. The t-batch algorithm is also an interesting way to optimise training. In my prior experience, training models on sequential data poses unique challenges, as we saw above, so it is interesting to see how some methods address them.\n\nReferences\n\n\nKumar, Srijan, Xikun Zhang, and Jure Leskovec. 2019. “Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks.” In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &Amp; Data Mining. KDD ’19. ACM. https://doi.org/10.1145/3292500.3330895."
  },
  {
    "objectID": "sessions/graphs-and-sets.html",
    "href": "sessions/graphs-and-sets.html",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "",
    "text": "Both a graph and a set provide a structure that is easy to analyze:\n\nDomain is discrete (nodes or nodes + edges)\nMinimal geometric assumptions [there was a comment on “resistance to permutations” that I didn’t get]\n\n\n\nBasically anything! Fun example: tube map 😊\nAlso google maps: optimal route from A —&gt; B probably goes through a graph neural network."
  },
  {
    "objectID": "sessions/graphs-and-sets.html#why-are-graphssets-a-useful-blueprint-for-geometric-deep-learning",
    "href": "sessions/graphs-and-sets.html#why-are-graphssets-a-useful-blueprint-for-geometric-deep-learning",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "",
    "text": "Both a graph and a set provide a structure that is easy to analyze:\n\nDomain is discrete (nodes or nodes + edges)\nMinimal geometric assumptions [there was a comment on “resistance to permutations” that I didn’t get]\n\n\n\nBasically anything! Fun example: tube map 😊\nAlso google maps: optimal route from A —&gt; B probably goes through a graph neural network."
  },
  {
    "objectID": "sessions/graphs-and-sets.html#first-step-graphs-without-edges-sets",
    "href": "sessions/graphs-and-sets.html#first-step-graphs-without-edges-sets",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "First step: graphs without edges (sets)",
    "text": "First step: graphs without edges (sets)\nj Useful for point-cloud like structures, unordered collections of objects. We can begin by defining a set of \\(n\\) nodes, each with a feature vector \\(\\mathbf{x}_i\\) of length \\(v\\), giving us the \\(n \\times v\\) feature matrix \\(\\mathbf{X}\\), where every row is a set of features for one node:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n           \\mathbf{x}_{1} \\\\\n           \\mathbf{x}_{2} \\\\\n           \\vdots \\\\\n           \\mathbf{x}_{n}\n         \\end{bmatrix} ~.\n\\]\nAh, but wait: I’ve chosen to number the nodes 1 through \\(n\\) right? That means I’ve defined an ordering! We need to make sure that the result of any calculation involving \\(\\mathbf{}\\)\\(\\mathbf{X}\\) is invariant to the ordering of the nodes, since we want to treat this as an unordered collection.\nAnother way of putting this is that we want the result of applying our calculation to be equal for all possible orderings of \\(\\mathbf{X}\\):\n\n\n\nUntitled"
  },
  {
    "objectID": "sessions/graphs-and-sets.html#why-are-graphssets-a-useful-blueprint-for-geometric-deep-learning-1",
    "href": "sessions/graphs-and-sets.html#why-are-graphssets-a-useful-blueprint-for-geometric-deep-learning-1",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "Why are graphs/sets a useful blueprint for “geometric” deep learning?",
    "text": "Why are graphs/sets a useful blueprint for “geometric” deep learning?\nBoth a graph and a set provide a structure that is easy to analyze:\n\nDomain is discrete (nodes or nodes + edges)\nMinimal geometric assumptions [there was a comment on “resistance to permutations” that I didn’t get]\n\n\nWhat kind of data is graph-like?\nBasically anything! Fun example: tube map 😊\nAlso google maps: optimal route from A —&gt; B probably goes through a graph neural network."
  },
  {
    "objectID": "sessions/graphs-and-sets.html#first-step-graphs-without-edges-sets-1",
    "href": "sessions/graphs-and-sets.html#first-step-graphs-without-edges-sets-1",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "First step: graphs without edges (sets)",
    "text": "First step: graphs without edges (sets)\nj Useful for point-cloud like structures, unordered collections of objects. We can begin by defining a set of \\(n\\) nodes, each with a feature vector \\(\\mathbf{x}_i\\) of length \\(v\\), giving us the \\(n \\times v\\) feature matrix \\(\\mathbf{X}\\), where every row is a set of features for one node:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n           \\mathbf{x}_{1} \\\\\n           \\mathbf{x}_{2} \\\\\n           \\vdots \\\\\n           \\mathbf{x}_{n}\n         \\end{bmatrix} ~.\n\\]\nAh, but wait: I’ve chosen to number the nodes 1 through \\(n\\) right? That means I’ve defined an ordering! We need to make sure that the result of any calculation involving \\(\\mathbf{}\\)\\(\\mathbf{X}\\) is invariant to the ordering of the nodes, since we want to treat this as an unordered collection.\nAnother way of putting this is that we want the result of applying our calculation to be equal for all possible orderings of \\(\\mathbf{X}\\):\n\n\n\nUntitled\n\n\nThis is a function \\(f\\) applied to two different orderings, or permutations of nodes, and we’re requiring that they’re equal in output. To permute the nodes is to change their order; we want this condition of equality to hold over all possible permutations.\nWe can write permutations of \\(\\mathbf{X}\\) as the product of \\(\\mathbf{X}\\) with some permutation matrix \\(\\mathbf{P}\\), which is square in the feature dimension \\(v\\), and has exactly one 1 in each row and column. The only result of applying \\(\\mathbf{P}\\) is the reordering of the feature vector order. Example:\n\n\n\nUntitled\n\n\nGiven this, we can summarise permutation invariance through the following equation:\nPermutation invariance of \\(f\\): For any permutation matrix \\(\\mathbf{P}\\), we require \\(f(\\mathbf{P}\\mathbf{X}) = f(\\mathbf{X})\\). The output is unaffected by re-ordering the input.\n\nExample: Deep Sets\nGiven two learnable functions \\(\\phi,\\psi\\), the deep sets architecture — proposed initially in 2018 — has the following choice of \\(f\\):\n\\[\nf(\\mathbf{X}) = \\phi\\left(\\sum_{i=1}^{v}\\psi(\\mathbf{x}_i)\\right),\n\\]\nwhere summing the learned functions for each node enforces the permutation invariance property (it doesn’t matter what order you sum — you always get the same result). A concrete example of this would be to choose both \\(\\phi\\) and \\(\\psi\\) to be feed-forward neural networks (also called MLPs/multi-layer perceptrons).\nNote that summing here is just one choice of information aggregation — we could have just as easily chosen the maximum, empirical mean etc. The point is that all of these operations are invariant to the order of the input. The optimal way to aggregate information is a design choice, and may vary depending on the problem you’re solving and the properties you want to learn.\nIn future, we’ll use \\(\\bigoplus\\) to denote a general aggregator function, which needs to be chosen when actually implementing the architecture involving it."
  },
  {
    "objectID": "sessions/graphs-and-sets.html#node-level-reasoning-and-equivariance",
    "href": "sessions/graphs-and-sets.html#node-level-reasoning-and-equivariance",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "Node-level reasoning and equivariance",
    "text": "Node-level reasoning and equivariance\nYou’ll notice that if we choose to aggregate over nodes, we’re actually losing information about each node individually in our output — we can only make a statement about the set as a whole. In some applications, this may not be desirable behaviour; one could imagine wanting to make a classification statement about each node in your graph, or trying to predict a certain quantity on a per-node basis. How do we reconcile this with the notion of not caring about how the input is ordered?\nWe can zoom out a bit and recognise why we cared about permutation invariance at all: it was to ensure that the set was not accidentally treated as an ordered sequence, which may then propagate an ordering bias into the result of our calculations. However, perhaps there’s a notion of this that we’re happy with when predicting per-node quantities, since we’re more interested in the individual nodes than we are the whole set. If we are able to link each output to each node, then that should be enough to satisfy us, provided that this holds no matter how we shuffle the input.\nTo formalise this: For any permutation matrix \\(\\mathbf{P}\\), we want to be able to apply \\(\\mathbf{P}\\) to the input, and still be able to link each output to the right node, i.e. \\(f(\\mathbf{X})\\) should also change in the same way. For a general operation, this property is called equivariance: the output changes in the same way as the input if we apply an operation to just the input. Equivalently, we could say that if we applied the operation to the output, the result will be as if we did so for the input. For the case of permutations, we can write the following:\nPermutation equivariance of \\(f\\): For any permutation matrix \\(\\mathbf{P}\\), we require \\(f(\\mathbf{P}\\mathbf{X}) = \\mathbf{P}f(\\mathbf{X})\\). The output changes in the same way as the input.\n\nAside: Locality as a constraint\nImagine wanting to predict a label on an image, but wanting to stay robust to translations of that image. We could either force the network to learn this property by adding translation as a data augmentation, or we could build it directly into the architecture somehow, e.g. by pooling operations in CNNs (which are equivariant to spacial translations — the pooled values move in-step with the image).\nIn practice, it’s highly likely that a pure translation of the image is not the only thing to worry about. As an example, imagine taking a picture of a house from two different angles. Now you suddenly have a slightly different shape, and maybe a bird is on the roof in the second picture! We’d like to be robust, then, to not just shifts, but also to any deformations of the input that come along for the ride (see image from slides below).\n\n\n\nUntitled\n\n\nPossible solution: compose many small local operations, but do this very deep (e.g. small kernels in CNNs, but many layers). Local operations should not propagate any errors to the global picture. (?)\n\n\nHow do we enforce locality for equivariant functions on sets?\nAn easy way to retain equivariance (one node input links to one output) and locality (learning happens on a small set of nodes) is to just operate on each node individually — that is, we apply the same function \\(\\psi\\) to each element separately, and get a set of latents \\(\\mathbf{h}_i\\).\n\n\n\nUntitled\n\n\nThis might sound familiar — it’s the inner part of the Deep Sets architecture (before aggregating with the sum)."
  },
  {
    "objectID": "sessions/graphs-and-sets.html#learning-on-sets-summary",
    "href": "sessions/graphs-and-sets.html#learning-on-sets-summary",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "Learning on sets: summary",
    "text": "Learning on sets: summary\nRecall that sets are, by definition, an unordered set of objects, and we’re operating in a way that preserves this behaviour.\nNode-level learning: To learn local structure in a set fråom each node while respecting permutation equivariance, we construct a latent vector \\(\\mathbf{h}_i\\) from each \\(\\mathbf{x}_i\\) by applying the same learnable function \\(\\psi\\) to each node, and stack the results:\n\\[\n\\mathbf{h}_i = \\psi\\left(\\mathbf{x}_i\\right) ; ~~~ \\mathbf{H} = \\begin{bmatrix}\n           \\mathbf{h}_{1} \\\\\n           \\mathbf{h}_{2} \\\\\n           \\vdots \\\\\n           \\mathbf{h}_{n}\n         \\end{bmatrix} ~.\n\\]\nSet-level learning: We can generalise the above to sets by aggregating the latent vectors, which is permutation invariant with respect to the input nodes. Then (q: how much worse is it if we don’t?), we apply a second learnable function \\(\\phi\\) to arrive at the Deep Sets architecture:\n\\[\nf(\\mathbf{X}) = \\phi \\left(\\bigoplus_i \\psi \\left(\\mathbf{x}_i\\right)\\right)\n\\]\n\nIs Deep Sets the only way for learning on sets?\nFor many sets, apparently so! There are proofs referenced in the lectures that state that any learnable function that’s permutation invariant on sets can be reduced to the same expressivity as Deep Sets. Example: PointNet.\nThis is a function \\(f\\) applied to two different orderings, or permutations of nodes, and we’re requiring that they’re equal in output. To permute the nodes is to change their order; we want this condition of equality to hold over all possible permutations.\nWe can write permutations of \\(\\mathbf{X}\\) as the product of \\(\\mathbf{X}\\) with some permutation matrix \\(\\mathbf{P}\\), which is square in the feature dimension \\(v\\), and has exactly one 1 in each row and column. The only result of applying \\(\\mathbf{P}\\) is the reordering of the feature vector order. Example:\n\n\n\nUntitled\n\n\nGiven this, we can summarise permutation invariance through the following equation:\nPermutation invariance of \\(f\\): For any permutation matrix \\(\\mathbf{P}\\), we require \\(f(\\mathbf{P}\\mathbf{X}) = f(\\mathbf{X})\\). The output is unaffected by re-ordering the input.\n\n\nExample: Deep Sets\nGiven two learnable functions \\(\\phi,\\psi\\), the deep sets architecture — proposed initially in 2018 — has the following choice of \\(f\\):\n\\[\nf(\\mathbf{X}) = \\phi\\left(\\sum_{i=1}^{v}\\psi(\\mathbf{x}_i)\\right),\n\\]\nwhere summing the learned functions for each node enforces the permutation invariance property (it doesn’t matter what order you sum — you always get the same result). A concrete example of this would be to choose both \\(\\phi\\) and \\(\\psi\\) to be feed-forward neural networks (also called MLPs/multi-layer perceptrons).\nNote that summing here is just one choice of information aggregation — we could have just as easily chosen the maximum, empirical mean etc. The point is that all of these operations are invariant to the order of the input. The optimal way to aggregate information is a design choice, and may vary depending on the problem you’re solving and the properties you want to learn.\nIn future, we’ll use \\(\\bigoplus\\) to denote a general aggregator function, which needs to be chosen when actually implementing the architecture involving it."
  },
  {
    "objectID": "sessions/graphs-and-sets.html#node-level-reasoning-and-equivariance-1",
    "href": "sessions/graphs-and-sets.html#node-level-reasoning-and-equivariance-1",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "Node-level reasoning and equivariance",
    "text": "Node-level reasoning and equivariance\nYou’ll notice that if we choose to aggregate over nodes, we’re actually losing information about each node individually in our output — we can only make a statement about the set as a whole. In some applications, this may not be desirable behaviour; one could imagine wanting to make a classification statement about each node in your graph, or trying to predict a certain quantity on a per-node basis. How do we reconcile this with the notion of not caring about how the input is ordered?\nWe can zoom out a bit and recognise why we cared about permutation invariance at all: it was to ensure that the set was not accidentally treated as an ordered sequence, which may then propagate an ordering bias into the result of our calculations. However, perhaps there’s a notion of this that we’re happy with when predicting per-node quantities, since we’re more interested in the individual nodes than we are the whole set. If we are able to link each output to each node, then that should be enough to satisfy us, provided that this holds no matter how we shuffle the input.\nTo formalise this: For any permutation matrix \\(\\mathbf{P}\\), we want to be able to apply \\(\\mathbf{P}\\) to the input, and still be able to link each output to the right node, i.e. \\(f(\\mathbf{X})\\) should also change in the same way. For a general operation, this property is called equivariance: the output changes in the same way as the input if we apply an operation to just the input. Equivalently, we could say that if we applied the operation to the output, the result will be as if we did so for the input. For the case of permutations, we can write the following:\nPermutation equivariance of \\(f\\): For any permutation matrix \\(\\mathbf{P}\\), we require \\(f(\\mathbf{P}\\mathbf{X}) = \\mathbf{P}f(\\mathbf{X})\\). The output changes in the same way as the input.\n\nAside: Locality as a constraint\nImagine wanting to predict a label on an image, but wanting to stay robust to translations of that image. We could either force the network to learn this property by adding translation as a data augmentation, or we could build it directly into the architecture somehow, e.g. by pooling operations in CNNs (which are equivariant to spacial translations — the pooled values move in-step with the image).\nIn practice, it’s highly likely that a pure translation of the image is not the only thing to worry about. As an example, imagine taking a picture of a house from two different angles. Now you suddenly have a slightly different shape, and maybe a bird is on the roof in the second picture! We’d like to be robust, then, to not just shifts, but also to any deformations of the input that come along for the ride (see image from slides below).\n\n\n\nUntitled\n\n\nPossible solution: compose many small local operations, but do this very deep (e.g. small kernels in CNNs, but many layers). Local operations should not propagate any errors to the global picture. (?)\n\n\nHow do we enforce locality for equivariant functions on sets?\nAn easy way to retain equivariance (one node input links to one output) and locality (learning happens on a small set of nodes) is to just operate on each node individually — that is, we apply the same function \\(\\psi\\) to each element separately, and get a set of latents \\(\\mathbf{h}_i\\).\n\n\n\nUntitled\n\n\nThis might sound familiar — it’s the inner part of the Deep Sets architecture (before aggregating with the sum)."
  },
  {
    "objectID": "sessions/graphs-and-sets.html#learning-on-sets-summary-1",
    "href": "sessions/graphs-and-sets.html#learning-on-sets-summary-1",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "Learning on sets: summary",
    "text": "Learning on sets: summary\nRecall that sets are, by definition, an unordered set of objects, and we’re operating in a way that preserves this behaviour.\nNode-level learning: To learn local structure in a set fråom each node while respecting permutation equivariance, we construct a latent vector \\(\\mathbf{h}_i\\) from each \\(\\mathbf{x}_i\\) by applying the same learnable function \\(\\psi\\) to each node, and stack the results:\n\\[\n\\mathbf{h}_i = \\psi\\left(\\mathbf{x}_i\\right) ; ~~~ \\mathbf{H} = \\begin{bmatrix}\n           \\mathbf{h}_{1} \\\\\n           \\mathbf{h}_{2} \\\\\n           \\vdots \\\\\n           \\mathbf{h}_{n}\n         \\end{bmatrix} ~.\n\\]\nSet-level learning: We can generalise the above to sets by aggregating the latent vectors, which is permutation invariant with respect to the input nodes. Then (q: how much worse is it if we don’t?), we apply a second learnable function \\(\\phi\\) to arrive at the Deep Sets architecture:\n\\[\nf(\\mathbf{X}) = \\phi \\left(\\bigoplus_i \\psi \\left(\\mathbf{x}_i\\right)\\right)\n\\]\n\nIs Deep Sets the only way for learning on sets?\nFor many sets, apparently so! There are proofs referenced in the lectures that state that any learnable function that’s permutation invariant on sets can be reduced to the same expressivity as Deep Sets. Example: PointNet."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nThis is a living book of notes for the graph neural network reading group at The Alan Turing Institute.\nTo contribute, please make a PR to the repository linked above after we’ve had a session on your chosen topic. Write your notes in any of the supported formats for Quarto (.qmd, .md, or .ipynb), and Quarto will do all of the legwork to publish it to this webpage!"
  }
]