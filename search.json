[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nThis is a living book of notes for the graph neural network reading group at The Alan Turing Institute.\nTo contribute, please make a PR to the repository linked above after we‚Äôve had a session on your chosen topic. Write your notes in any of the supported formats for Quarto (.qmd, .md, or .ipynb), and Quarto will do all of the legwork to publish it to this webpage!"
  },
  {
    "objectID": "graphs-and-sets.html",
    "href": "graphs-and-sets.html",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "",
    "text": "Both a graph and a set provide a structure that is easy to analyze:\n\nDomain is discrete (nodes or nodes + edges)\nMinimal geometric assumptions [there was a comment on ‚Äúresistance to permutations‚Äù that I didn‚Äôt get]\n\n\n\nBasically anything! Fun example: tube map üòä\nAlso google maps: optimal route from A ‚Äî&gt; B probably goes through a graph neural network."
  },
  {
    "objectID": "graphs-and-sets.html#why-are-graphssets-a-useful-blueprint-for-geometric-deep-learning",
    "href": "graphs-and-sets.html#why-are-graphssets-a-useful-blueprint-for-geometric-deep-learning",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "",
    "text": "Both a graph and a set provide a structure that is easy to analyze:\n\nDomain is discrete (nodes or nodes + edges)\nMinimal geometric assumptions [there was a comment on ‚Äúresistance to permutations‚Äù that I didn‚Äôt get]\n\n\n\nBasically anything! Fun example: tube map üòä\nAlso google maps: optimal route from A ‚Äî&gt; B probably goes through a graph neural network."
  },
  {
    "objectID": "graphs-and-sets.html#first-step-graphs-without-edges-sets",
    "href": "graphs-and-sets.html#first-step-graphs-without-edges-sets",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "First step: graphs without edges (sets)",
    "text": "First step: graphs without edges (sets)\nj Useful for point-cloud like structures, unordered collections of objects. We can begin by defining a set of \\(n\\) nodes, each with a feature vector \\(\\mathbf{x}_i\\) of length \\(v\\), giving us the \\(n \\times v\\) feature matrix \\(\\mathbf{X}\\), where every row is a set of features for one node:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n           \\mathbf{x}_{1} \\\\\n           \\mathbf{x}_{2} \\\\\n           \\vdots \\\\\n           \\mathbf{x}_{n}\n         \\end{bmatrix} ~.\n\\]\nAh, but wait: I‚Äôve chosen to number the nodes 1 through \\(n\\) right? That means I‚Äôve defined an ordering! We need to make sure that the result of any calculation involving \\(\\mathbf{}\\)\\(\\mathbf{X}\\) is invariant to the ordering of the nodes, since we want to treat this as an unordered collection.\nAnother way of putting this is that we want the result of applying our calculation to be equal for all possible orderings of \\(\\mathbf{X}\\):\n\n\n\nUntitled"
  },
  {
    "objectID": "graphs-and-sets.html#why-are-graphssets-a-useful-blueprint-for-geometric-deep-learning-1",
    "href": "graphs-and-sets.html#why-are-graphssets-a-useful-blueprint-for-geometric-deep-learning-1",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "Why are graphs/sets a useful blueprint for ‚Äúgeometric‚Äù deep learning?",
    "text": "Why are graphs/sets a useful blueprint for ‚Äúgeometric‚Äù deep learning?\nBoth a graph and a set provide a structure that is easy to analyze:\n\nDomain is discrete (nodes or nodes + edges)\nMinimal geometric assumptions [there was a comment on ‚Äúresistance to permutations‚Äù that I didn‚Äôt get]\n\n\nWhat kind of data is graph-like?\nBasically anything! Fun example: tube map üòä\nAlso google maps: optimal route from A ‚Äî&gt; B probably goes through a graph neural network."
  },
  {
    "objectID": "graphs-and-sets.html#first-step-graphs-without-edges-sets-1",
    "href": "graphs-and-sets.html#first-step-graphs-without-edges-sets-1",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "First step: graphs without edges (sets)",
    "text": "First step: graphs without edges (sets)\nj Useful for point-cloud like structures, unordered collections of objects. We can begin by defining a set of \\(n\\) nodes, each with a feature vector \\(\\mathbf{x}_i\\) of length \\(v\\), giving us the \\(n \\times v\\) feature matrix \\(\\mathbf{X}\\), where every row is a set of features for one node:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n           \\mathbf{x}_{1} \\\\\n           \\mathbf{x}_{2} \\\\\n           \\vdots \\\\\n           \\mathbf{x}_{n}\n         \\end{bmatrix} ~.\n\\]\nAh, but wait: I‚Äôve chosen to number the nodes 1 through \\(n\\) right? That means I‚Äôve defined an ordering! We need to make sure that the result of any calculation involving \\(\\mathbf{}\\)\\(\\mathbf{X}\\) is invariant to the ordering of the nodes, since we want to treat this as an unordered collection.\nAnother way of putting this is that we want the result of applying our calculation to be equal for all possible orderings of \\(\\mathbf{X}\\):\n\n\n\nUntitled\n\n\nThis is a function \\(f\\) applied to two different orderings, or permutations of nodes, and we‚Äôre requiring that they‚Äôre equal in output. To permute the nodes is to change their order; we want this condition of equality to hold over all possible permutations.\nWe can write permutations of \\(\\mathbf{X}\\) as the product of \\(\\mathbf{X}\\) with some permutation matrix \\(\\mathbf{P}\\), which is square in the feature dimension \\(v\\), and has exactly one 1 in each row and column. The only result of applying \\(\\mathbf{P}\\) is the reordering of the feature vector order. Example:\n\n\n\nUntitled\n\n\nGiven this, we can summarise permutation invariance through the following equation:\nPermutation invariance of \\(f\\): For any permutation matrix \\(\\mathbf{P}\\), we require \\(f(\\mathbf{P}\\mathbf{X}) = f(\\mathbf{X})\\). The output is unaffected by re-ordering the input.\n\nExample: Deep Sets\nGiven two learnable functions \\(\\phi,\\psi\\), the deep sets architecture ‚Äî proposed initially in 2018 ‚Äî has the following choice of \\(f\\):\n\\[\nf(\\mathbf{X}) = \\phi\\left(\\sum_{i=1}^{v}\\psi(\\mathbf{x}_i)\\right),\n\\]\nwhere summing the learned functions for each node enforces the permutation invariance property (it doesn‚Äôt matter what order you sum ‚Äî you always get the same result). A concrete example of this would be to choose both \\(\\phi\\) and \\(\\psi\\) to be feed-forward neural networks (also called MLPs/multi-layer perceptrons).\nNote that summing here is just one choice of information aggregation ‚Äî we could have just as easily chosen the maximum, empirical mean etc. The point is that all of these operations are invariant to the order of the input. The optimal way to aggregate information is a design choice, and may vary depending on the problem you‚Äôre solving and the properties you want to learn.\nIn future, we‚Äôll use \\(\\bigoplus\\) to denote a general aggregator function, which needs to be chosen when actually implementing the architecture involving it."
  },
  {
    "objectID": "graphs-and-sets.html#node-level-reasoning-and-equivariance",
    "href": "graphs-and-sets.html#node-level-reasoning-and-equivariance",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "Node-level reasoning and equivariance",
    "text": "Node-level reasoning and equivariance\nYou‚Äôll notice that if we choose to aggregate over nodes, we‚Äôre actually losing information about each node individually in our output ‚Äî we can only make a statement about the set as a whole. In some applications, this may not be desirable behaviour; one could imagine wanting to make a classification statement about each node in your graph, or trying to predict a certain quantity on a per-node basis. How do we reconcile this with the notion of not caring about how the input is ordered?\nWe can zoom out a bit and recognise why we cared about permutation invariance at all: it was to ensure that the set was not accidentally treated as an ordered sequence, which may then propagate an ordering bias into the result of our calculations. However, perhaps there‚Äôs a notion of this that we‚Äôre happy with when predicting per-node quantities, since we‚Äôre more interested in the individual nodes than we are the whole set. If we are able to link each output to each node, then that should be enough to satisfy us, provided that this holds no matter how we shuffle the input.\nTo formalise this: For any permutation matrix \\(\\mathbf{P}\\), we want to be able to apply \\(\\mathbf{P}\\) to the input, and still be able to link each output to the right node, i.e.¬†\\(f(\\mathbf{X})\\) should also change in the same way. For a general operation, this property is called equivariance: the output changes in the same way as the input if we apply an operation to just the input. Equivalently, we could say that if we applied the operation to the output, the result will be as if we did so for the input. For the case of permutations, we can write the following:\nPermutation equivariance of \\(f\\): For any permutation matrix \\(\\mathbf{P}\\), we require \\(f(\\mathbf{P}\\mathbf{X}) = \\mathbf{P}f(\\mathbf{X})\\). The output changes in the same way as the input.\n\nAside: Locality as a constraint\nImagine wanting to predict a label on an image, but wanting to stay robust to translations of that image. We could either force the network to learn this property by adding translation as a data augmentation, or we could build it directly into the architecture somehow, e.g.¬†by pooling operations in CNNs (which are equivariant to spacial translations ‚Äî the pooled values move in-step with the image).\nIn practice, it‚Äôs highly likely that a pure translation of the image is not the only thing to worry about. As an example, imagine taking a picture of a house from two different angles. Now you suddenly have a slightly different shape, and maybe a bird is on the roof in the second picture! We‚Äôd like to be robust, then, to not just shifts, but also to any deformations of the input that come along for the ride (see image from slides below).\n\n\n\nUntitled\n\n\nPossible solution: compose many small local operations, but do this very deep (e.g.¬†small kernels in CNNs, but many layers). Local operations should not propagate any errors to the global picture. (?)\n\n\nHow do we enforce locality for equivariant functions on sets?\nAn easy way to retain equivariance (one node input links to one output) and locality (learning happens on a small set of nodes) is to just operate on each node individually ‚Äî that is, we apply the same function \\(\\psi\\) to each element separately, and get a set of latents \\(\\mathbf{h}_i\\).\n\n\n\nUntitled\n\n\nThis might sound familiar ‚Äî it‚Äôs the inner part of the Deep Sets architecture (before aggregating with the sum)."
  },
  {
    "objectID": "graphs-and-sets.html#learning-on-sets-summary",
    "href": "graphs-and-sets.html#learning-on-sets-summary",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "Learning on sets: summary",
    "text": "Learning on sets: summary\nRecall that sets are, by definition, an unordered set of objects, and we‚Äôre operating in a way that preserves this behaviour.\nNode-level learning: To learn local structure in a set fr√•om each node while respecting permutation equivariance, we construct a latent vector \\(\\mathbf{h}_i\\) from each \\(\\mathbf{x}_i\\) by applying the same learnable function \\(\\psi\\) to each node, and stack the results:\n\\[\n\\mathbf{h}_i = \\psi\\left(\\mathbf{x}_i\\right) ; ~~~ \\mathbf{H} = \\begin{bmatrix}\n           \\mathbf{h}_{1} \\\\\n           \\mathbf{h}_{2} \\\\\n           \\vdots \\\\\n           \\mathbf{h}_{n}\n         \\end{bmatrix} ~.\n\\]\nSet-level learning: We can generalise the above to sets by aggregating the latent vectors, which is permutation invariant with respect to the input nodes. Then (q: how much worse is it if we don‚Äôt?), we apply a second learnable function \\(\\phi\\) to arrive at the Deep Sets architecture:\n\\[\nf(\\mathbf{X}) = \\phi \\left(\\bigoplus_i \\psi \\left(\\mathbf{x}_i\\right)\\right)\n\\]\n\nIs Deep Sets the only way for learning on sets?\nFor many sets, apparently so! There are proofs referenced in the lectures that state that any learnable function that‚Äôs permutation invariant on sets can be reduced to the same expressivity as Deep Sets. Example: PointNet.\nThis is a function \\(f\\) applied to two different orderings, or permutations of nodes, and we‚Äôre requiring that they‚Äôre equal in output. To permute the nodes is to change their order; we want this condition of equality to hold over all possible permutations.\nWe can write permutations of \\(\\mathbf{X}\\) as the product of \\(\\mathbf{X}\\) with some permutation matrix \\(\\mathbf{P}\\), which is square in the feature dimension \\(v\\), and has exactly one 1 in each row and column. The only result of applying \\(\\mathbf{P}\\) is the reordering of the feature vector order. Example:\n\n\n\nUntitled\n\n\nGiven this, we can summarise permutation invariance through the following equation:\nPermutation invariance of \\(f\\): For any permutation matrix \\(\\mathbf{P}\\), we require \\(f(\\mathbf{P}\\mathbf{X}) = f(\\mathbf{X})\\). The output is unaffected by re-ordering the input.\n\n\nExample: Deep Sets\nGiven two learnable functions \\(\\phi,\\psi\\), the deep sets architecture ‚Äî proposed initially in 2018 ‚Äî has the following choice of \\(f\\):\n\\[\nf(\\mathbf{X}) = \\phi\\left(\\sum_{i=1}^{v}\\psi(\\mathbf{x}_i)\\right),\n\\]\nwhere summing the learned functions for each node enforces the permutation invariance property (it doesn‚Äôt matter what order you sum ‚Äî you always get the same result). A concrete example of this would be to choose both \\(\\phi\\) and \\(\\psi\\) to be feed-forward neural networks (also called MLPs/multi-layer perceptrons).\nNote that summing here is just one choice of information aggregation ‚Äî we could have just as easily chosen the maximum, empirical mean etc. The point is that all of these operations are invariant to the order of the input. The optimal way to aggregate information is a design choice, and may vary depending on the problem you‚Äôre solving and the properties you want to learn.\nIn future, we‚Äôll use \\(\\bigoplus\\) to denote a general aggregator function, which needs to be chosen when actually implementing the architecture involving it."
  },
  {
    "objectID": "graphs-and-sets.html#node-level-reasoning-and-equivariance-1",
    "href": "graphs-and-sets.html#node-level-reasoning-and-equivariance-1",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "Node-level reasoning and equivariance",
    "text": "Node-level reasoning and equivariance\nYou‚Äôll notice that if we choose to aggregate over nodes, we‚Äôre actually losing information about each node individually in our output ‚Äî we can only make a statement about the set as a whole. In some applications, this may not be desirable behaviour; one could imagine wanting to make a classification statement about each node in your graph, or trying to predict a certain quantity on a per-node basis. How do we reconcile this with the notion of not caring about how the input is ordered?\nWe can zoom out a bit and recognise why we cared about permutation invariance at all: it was to ensure that the set was not accidentally treated as an ordered sequence, which may then propagate an ordering bias into the result of our calculations. However, perhaps there‚Äôs a notion of this that we‚Äôre happy with when predicting per-node quantities, since we‚Äôre more interested in the individual nodes than we are the whole set. If we are able to link each output to each node, then that should be enough to satisfy us, provided that this holds no matter how we shuffle the input.\nTo formalise this: For any permutation matrix \\(\\mathbf{P}\\), we want to be able to apply \\(\\mathbf{P}\\) to the input, and still be able to link each output to the right node, i.e.¬†\\(f(\\mathbf{X})\\) should also change in the same way. For a general operation, this property is called equivariance: the output changes in the same way as the input if we apply an operation to just the input. Equivalently, we could say that if we applied the operation to the output, the result will be as if we did so for the input. For the case of permutations, we can write the following:\nPermutation equivariance of \\(f\\): For any permutation matrix \\(\\mathbf{P}\\), we require \\(f(\\mathbf{P}\\mathbf{X}) = \\mathbf{P}f(\\mathbf{X})\\). The output changes in the same way as the input.\n\nAside: Locality as a constraint\nImagine wanting to predict a label on an image, but wanting to stay robust to translations of that image. We could either force the network to learn this property by adding translation as a data augmentation, or we could build it directly into the architecture somehow, e.g.¬†by pooling operations in CNNs (which are equivariant to spacial translations ‚Äî the pooled values move in-step with the image).\nIn practice, it‚Äôs highly likely that a pure translation of the image is not the only thing to worry about. As an example, imagine taking a picture of a house from two different angles. Now you suddenly have a slightly different shape, and maybe a bird is on the roof in the second picture! We‚Äôd like to be robust, then, to not just shifts, but also to any deformations of the input that come along for the ride (see image from slides below).\n\n\n\nUntitled\n\n\nPossible solution: compose many small local operations, but do this very deep (e.g.¬†small kernels in CNNs, but many layers). Local operations should not propagate any errors to the global picture. (?)\n\n\nHow do we enforce locality for equivariant functions on sets?\nAn easy way to retain equivariance (one node input links to one output) and locality (learning happens on a small set of nodes) is to just operate on each node individually ‚Äî that is, we apply the same function \\(\\psi\\) to each element separately, and get a set of latents \\(\\mathbf{h}_i\\).\n\n\n\nUntitled\n\n\nThis might sound familiar ‚Äî it‚Äôs the inner part of the Deep Sets architecture (before aggregating with the sum)."
  },
  {
    "objectID": "graphs-and-sets.html#learning-on-sets-summary-1",
    "href": "graphs-and-sets.html#learning-on-sets-summary-1",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "Learning on sets: summary",
    "text": "Learning on sets: summary\nRecall that sets are, by definition, an unordered set of objects, and we‚Äôre operating in a way that preserves this behaviour.\nNode-level learning: To learn local structure in a set fr√•om each node while respecting permutation equivariance, we construct a latent vector \\(\\mathbf{h}_i\\) from each \\(\\mathbf{x}_i\\) by applying the same learnable function \\(\\psi\\) to each node, and stack the results:\n\\[\n\\mathbf{h}_i = \\psi\\left(\\mathbf{x}_i\\right) ; ~~~ \\mathbf{H} = \\begin{bmatrix}\n           \\mathbf{h}_{1} \\\\\n           \\mathbf{h}_{2} \\\\\n           \\vdots \\\\\n           \\mathbf{h}_{n}\n         \\end{bmatrix} ~.\n\\]\nSet-level learning: We can generalise the above to sets by aggregating the latent vectors, which is permutation invariant with respect to the input nodes. Then (q: how much worse is it if we don‚Äôt?), we apply a second learnable function \\(\\phi\\) to arrive at the Deep Sets architecture:\n\\[\nf(\\mathbf{X}) = \\phi \\left(\\bigoplus_i \\psi \\left(\\mathbf{x}_i\\right)\\right)\n\\]\n\nIs Deep Sets the only way for learning on sets?\nFor many sets, apparently so! There are proofs referenced in the lectures that state that any learnable function that‚Äôs permutation invariant on sets can be reduced to the same expressivity as Deep Sets. Example: PointNet."
  }
]