[
  {
    "objectID": "graphs-and-sets.html",
    "href": "graphs-and-sets.html",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "",
    "text": "Both a graph and a set provide a structure that is easy to analyze:\n\nDomain is discrete (nodes or nodes + edges)\nMinimal geometric assumptions [there was a comment on ‚Äúresistance to permutations‚Äù that I didn‚Äôt get]\n\n\n\nBasically anything! Fun example: tube map üòä\nAlso google maps: optimal route from A ‚Äî&gt; B probably goes through a graph neural network."
  },
  {
    "objectID": "graphs-and-sets.html#why-are-graphssets-a-useful-blueprint-for-geometric-deep-learning",
    "href": "graphs-and-sets.html#why-are-graphssets-a-useful-blueprint-for-geometric-deep-learning",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "",
    "text": "Both a graph and a set provide a structure that is easy to analyze:\n\nDomain is discrete (nodes or nodes + edges)\nMinimal geometric assumptions [there was a comment on ‚Äúresistance to permutations‚Äù that I didn‚Äôt get]\n\n\n\nBasically anything! Fun example: tube map üòä\nAlso google maps: optimal route from A ‚Äî&gt; B probably goes through a graph neural network."
  },
  {
    "objectID": "graphs-and-sets.html#first-step-graphs-without-edges-sets",
    "href": "graphs-and-sets.html#first-step-graphs-without-edges-sets",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "First step: graphs without edges (sets)",
    "text": "First step: graphs without edges (sets)\nj Useful for point-cloud like structures, unordered collections of objects. We can begin by defining a set of \\(n\\) nodes, each with a feature vector \\(\\mathbf{x}_i\\) of length \\(v\\), giving us the \\(n \\times v\\) feature matrix \\(\\mathbf{X}\\), where every row is a set of features for one node:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n           \\mathbf{x}_{1} \\\\\n           \\mathbf{x}_{2} \\\\\n           \\vdots \\\\\n           \\mathbf{x}_{n}\n         \\end{bmatrix} ~.\n\\]\nAh, but wait: I‚Äôve chosen to number the nodes 1 through \\(n\\) right? That means I‚Äôve defined an ordering! We need to make sure that the result of any calculation involving \\(\\mathbf{}\\)\\(\\mathbf{X}\\) is invariant to the ordering of the nodes, since we want to treat this as an unordered collection.\nAnother way of putting this is that we want the result of applying our calculation to be equal for all possible orderings of \\(\\mathbf{X}\\):\n\n\n\nUntitled"
  },
  {
    "objectID": "graphs-and-sets.html#why-are-graphssets-a-useful-blueprint-for-geometric-deep-learning-1",
    "href": "graphs-and-sets.html#why-are-graphssets-a-useful-blueprint-for-geometric-deep-learning-1",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "Why are graphs/sets a useful blueprint for ‚Äúgeometric‚Äù deep learning?",
    "text": "Why are graphs/sets a useful blueprint for ‚Äúgeometric‚Äù deep learning?\nBoth a graph and a set provide a structure that is easy to analyze:\n\nDomain is discrete (nodes or nodes + edges)\nMinimal geometric assumptions [there was a comment on ‚Äúresistance to permutations‚Äù that I didn‚Äôt get]\n\n\nWhat kind of data is graph-like?\nBasically anything! Fun example: tube map üòä\nAlso google maps: optimal route from A ‚Äî&gt; B probably goes through a graph neural network."
  },
  {
    "objectID": "graphs-and-sets.html#first-step-graphs-without-edges-sets-1",
    "href": "graphs-and-sets.html#first-step-graphs-without-edges-sets-1",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "First step: graphs without edges (sets)",
    "text": "First step: graphs without edges (sets)\nj Useful for point-cloud like structures, unordered collections of objects. We can begin by defining a set of \\(n\\) nodes, each with a feature vector \\(\\mathbf{x}_i\\) of length \\(v\\), giving us the \\(n \\times v\\) feature matrix \\(\\mathbf{X}\\), where every row is a set of features for one node:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n           \\mathbf{x}_{1} \\\\\n           \\mathbf{x}_{2} \\\\\n           \\vdots \\\\\n           \\mathbf{x}_{n}\n         \\end{bmatrix} ~.\n\\]\nAh, but wait: I‚Äôve chosen to number the nodes 1 through \\(n\\) right? That means I‚Äôve defined an ordering! We need to make sure that the result of any calculation involving \\(\\mathbf{}\\)\\(\\mathbf{X}\\) is invariant to the ordering of the nodes, since we want to treat this as an unordered collection.\nAnother way of putting this is that we want the result of applying our calculation to be equal for all possible orderings of \\(\\mathbf{X}\\):\n\n\n\nUntitled\n\n\nThis is a function \\(f\\) applied to two different orderings, or permutations of nodes, and we‚Äôre requiring that they‚Äôre equal in output. To permute the nodes is to change their order; we want this condition of equality to hold over all possible permutations.\nWe can write permutations of \\(\\mathbf{X}\\) as the product of \\(\\mathbf{X}\\) with some permutation matrix \\(\\mathbf{P}\\), which is square in the feature dimension \\(v\\), and has exactly one 1 in each row and column. The only result of applying \\(\\mathbf{P}\\) is the reordering of the feature vector order. Example:\n\n\n\nUntitled\n\n\nGiven this, we can summarise permutation invariance through the following equation:\nPermutation invariance of \\(f\\): For any permutation matrix \\(\\mathbf{P}\\), we require \\(f(\\mathbf{P}\\mathbf{X}) = f(\\mathbf{X})\\). The output is unaffected by re-ordering the input.\n\nExample: Deep Sets\nGiven two learnable functions \\(\\phi,\\psi\\), the deep sets architecture ‚Äî proposed initially in 2018 ‚Äî has the following choice of \\(f\\):\n\\[\nf(\\mathbf{X}) = \\phi\\left(\\sum_{i=1}^{v}\\psi(\\mathbf{x}_i)\\right),\n\\]\nwhere summing the learned functions for each node enforces the permutation invariance property (it doesn‚Äôt matter what order you sum ‚Äî you always get the same result). A concrete example of this would be to choose both \\(\\phi\\) and \\(\\psi\\) to be feed-forward neural networks (also called MLPs/multi-layer perceptrons).\nNote that summing here is just one choice of information aggregation ‚Äî we could have just as easily chosen the maximum, empirical mean etc. The point is that all of these operations are invariant to the order of the input. The optimal way to aggregate information is a design choice, and may vary depending on the problem you‚Äôre solving and the properties you want to learn.\nIn future, we‚Äôll use \\(\\bigoplus\\) to denote a general aggregator function, which needs to be chosen when actually implementing the architecture involving it."
  },
  {
    "objectID": "graphs-and-sets.html#node-level-reasoning-and-equivariance",
    "href": "graphs-and-sets.html#node-level-reasoning-and-equivariance",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "Node-level reasoning and equivariance",
    "text": "Node-level reasoning and equivariance\nYou‚Äôll notice that if we choose to aggregate over nodes, we‚Äôre actually losing information about each node individually in our output ‚Äî we can only make a statement about the set as a whole. In some applications, this may not be desirable behaviour; one could imagine wanting to make a classification statement about each node in your graph, or trying to predict a certain quantity on a per-node basis. How do we reconcile this with the notion of not caring about how the input is ordered?\nWe can zoom out a bit and recognise why we cared about permutation invariance at all: it was to ensure that the set was not accidentally treated as an ordered sequence, which may then propagate an ordering bias into the result of our calculations. However, perhaps there‚Äôs a notion of this that we‚Äôre happy with when predicting per-node quantities, since we‚Äôre more interested in the individual nodes than we are the whole set. If we are able to link each output to each node, then that should be enough to satisfy us, provided that this holds no matter how we shuffle the input.\nTo formalise this: For any permutation matrix \\(\\mathbf{P}\\), we want to be able to apply \\(\\mathbf{P}\\) to the input, and still be able to link each output to the right node, i.e.¬†\\(f(\\mathbf{X})\\) should also change in the same way. For a general operation, this property is called equivariance: the output changes in the same way as the input if we apply an operation to just the input. Equivalently, we could say that if we applied the operation to the output, the result will be as if we did so for the input. For the case of permutations, we can write the following:\nPermutation equivariance of \\(f\\): For any permutation matrix \\(\\mathbf{P}\\), we require \\(f(\\mathbf{P}\\mathbf{X}) = \\mathbf{P}f(\\mathbf{X})\\). The output changes in the same way as the input.\n\nAside: Locality as a constraint\nImagine wanting to predict a label on an image, but wanting to stay robust to translations of that image. We could either force the network to learn this property by adding translation as a data augmentation, or we could build it directly into the architecture somehow, e.g.¬†by pooling operations in CNNs (which are equivariant to spacial translations ‚Äî the pooled values move in-step with the image).\nIn practice, it‚Äôs highly likely that a pure translation of the image is not the only thing to worry about. As an example, imagine taking a picture of a house from two different angles. Now you suddenly have a slightly different shape, and maybe a bird is on the roof in the second picture! We‚Äôd like to be robust, then, to not just shifts, but also to any deformations of the input that come along for the ride (see image from slides below).\n\n\n\nUntitled\n\n\nPossible solution: compose many small local operations, but do this very deep (e.g.¬†small kernels in CNNs, but many layers). Local operations should not propagate any errors to the global picture. (?)\n\n\nHow do we enforce locality for equivariant functions on sets?\nAn easy way to retain equivariance (one node input links to one output) and locality (learning happens on a small set of nodes) is to just operate on each node individually ‚Äî that is, we apply the same function \\(\\psi\\) to each element separately, and get a set of latents \\(\\mathbf{h}_i\\).\n\n\n\nUntitled\n\n\nThis might sound familiar ‚Äî it‚Äôs the inner part of the Deep Sets architecture (before aggregating with the sum)."
  },
  {
    "objectID": "graphs-and-sets.html#learning-on-sets-summary",
    "href": "graphs-and-sets.html#learning-on-sets-summary",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "Learning on sets: summary",
    "text": "Learning on sets: summary\nRecall that sets are, by definition, an unordered set of objects, and we‚Äôre operating in a way that preserves this behaviour.\nNode-level learning: To learn local structure in a set fr√•om each node while respecting permutation equivariance, we construct a latent vector \\(\\mathbf{h}_i\\) from each \\(\\mathbf{x}_i\\) by applying the same learnable function \\(\\psi\\) to each node, and stack the results:\n\\[\n\\mathbf{h}_i = \\psi\\left(\\mathbf{x}_i\\right) ; ~~~ \\mathbf{H} = \\begin{bmatrix}\n           \\mathbf{h}_{1} \\\\\n           \\mathbf{h}_{2} \\\\\n           \\vdots \\\\\n           \\mathbf{h}_{n}\n         \\end{bmatrix} ~.\n\\]\nSet-level learning: We can generalise the above to sets by aggregating the latent vectors, which is permutation invariant with respect to the input nodes. Then (q: how much worse is it if we don‚Äôt?), we apply a second learnable function \\(\\phi\\) to arrive at the Deep Sets architecture:\n\\[\nf(\\mathbf{X}) = \\phi \\left(\\bigoplus_i \\psi \\left(\\mathbf{x}_i\\right)\\right)\n\\]\n\nIs Deep Sets the only way for learning on sets?\nFor many sets, apparently so! There are proofs referenced in the lectures that state that any learnable function that‚Äôs permutation invariant on sets can be reduced to the same expressivity as Deep Sets. Example: PointNet.\nThis is a function \\(f\\) applied to two different orderings, or permutations of nodes, and we‚Äôre requiring that they‚Äôre equal in output. To permute the nodes is to change their order; we want this condition of equality to hold over all possible permutations.\nWe can write permutations of \\(\\mathbf{X}\\) as the product of \\(\\mathbf{X}\\) with some permutation matrix \\(\\mathbf{P}\\), which is square in the feature dimension \\(v\\), and has exactly one 1 in each row and column. The only result of applying \\(\\mathbf{P}\\) is the reordering of the feature vector order. Example:\n\n\n\nUntitled\n\n\nGiven this, we can summarise permutation invariance through the following equation:\nPermutation invariance of \\(f\\): For any permutation matrix \\(\\mathbf{P}\\), we require \\(f(\\mathbf{P}\\mathbf{X}) = f(\\mathbf{X})\\). The output is unaffected by re-ordering the input.\n\n\nExample: Deep Sets\nGiven two learnable functions \\(\\phi,\\psi\\), the deep sets architecture ‚Äî proposed initially in 2018 ‚Äî has the following choice of \\(f\\):\n\\[\nf(\\mathbf{X}) = \\phi\\left(\\sum_{i=1}^{v}\\psi(\\mathbf{x}_i)\\right),\n\\]\nwhere summing the learned functions for each node enforces the permutation invariance property (it doesn‚Äôt matter what order you sum ‚Äî you always get the same result). A concrete example of this would be to choose both \\(\\phi\\) and \\(\\psi\\) to be feed-forward neural networks (also called MLPs/multi-layer perceptrons).\nNote that summing here is just one choice of information aggregation ‚Äî we could have just as easily chosen the maximum, empirical mean etc. The point is that all of these operations are invariant to the order of the input. The optimal way to aggregate information is a design choice, and may vary depending on the problem you‚Äôre solving and the properties you want to learn.\nIn future, we‚Äôll use \\(\\bigoplus\\) to denote a general aggregator function, which needs to be chosen when actually implementing the architecture involving it."
  },
  {
    "objectID": "graphs-and-sets.html#node-level-reasoning-and-equivariance-1",
    "href": "graphs-and-sets.html#node-level-reasoning-and-equivariance-1",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "Node-level reasoning and equivariance",
    "text": "Node-level reasoning and equivariance\nYou‚Äôll notice that if we choose to aggregate over nodes, we‚Äôre actually losing information about each node individually in our output ‚Äî we can only make a statement about the set as a whole. In some applications, this may not be desirable behaviour; one could imagine wanting to make a classification statement about each node in your graph, or trying to predict a certain quantity on a per-node basis. How do we reconcile this with the notion of not caring about how the input is ordered?\nWe can zoom out a bit and recognise why we cared about permutation invariance at all: it was to ensure that the set was not accidentally treated as an ordered sequence, which may then propagate an ordering bias into the result of our calculations. However, perhaps there‚Äôs a notion of this that we‚Äôre happy with when predicting per-node quantities, since we‚Äôre more interested in the individual nodes than we are the whole set. If we are able to link each output to each node, then that should be enough to satisfy us, provided that this holds no matter how we shuffle the input.\nTo formalise this: For any permutation matrix \\(\\mathbf{P}\\), we want to be able to apply \\(\\mathbf{P}\\) to the input, and still be able to link each output to the right node, i.e.¬†\\(f(\\mathbf{X})\\) should also change in the same way. For a general operation, this property is called equivariance: the output changes in the same way as the input if we apply an operation to just the input. Equivalently, we could say that if we applied the operation to the output, the result will be as if we did so for the input. For the case of permutations, we can write the following:\nPermutation equivariance of \\(f\\): For any permutation matrix \\(\\mathbf{P}\\), we require \\(f(\\mathbf{P}\\mathbf{X}) = \\mathbf{P}f(\\mathbf{X})\\). The output changes in the same way as the input.\n\nAside: Locality as a constraint\nImagine wanting to predict a label on an image, but wanting to stay robust to translations of that image. We could either force the network to learn this property by adding translation as a data augmentation, or we could build it directly into the architecture somehow, e.g.¬†by pooling operations in CNNs (which are equivariant to spacial translations ‚Äî the pooled values move in-step with the image).\nIn practice, it‚Äôs highly likely that a pure translation of the image is not the only thing to worry about. As an example, imagine taking a picture of a house from two different angles. Now you suddenly have a slightly different shape, and maybe a bird is on the roof in the second picture! We‚Äôd like to be robust, then, to not just shifts, but also to any deformations of the input that come along for the ride (see image from slides below).\n\n\n\nUntitled\n\n\nPossible solution: compose many small local operations, but do this very deep (e.g.¬†small kernels in CNNs, but many layers). Local operations should not propagate any errors to the global picture. (?)\n\n\nHow do we enforce locality for equivariant functions on sets?\nAn easy way to retain equivariance (one node input links to one output) and locality (learning happens on a small set of nodes) is to just operate on each node individually ‚Äî that is, we apply the same function \\(\\psi\\) to each element separately, and get a set of latents \\(\\mathbf{h}_i\\).\n\n\n\nUntitled\n\n\nThis might sound familiar ‚Äî it‚Äôs the inner part of the Deep Sets architecture (before aggregating with the sum)."
  },
  {
    "objectID": "graphs-and-sets.html#learning-on-sets-summary-1",
    "href": "graphs-and-sets.html#learning-on-sets-summary-1",
    "title": "Graphs and Sets (Geometric Deep Learning)",
    "section": "Learning on sets: summary",
    "text": "Learning on sets: summary\nRecall that sets are, by definition, an unordered set of objects, and we‚Äôre operating in a way that preserves this behaviour.\nNode-level learning: To learn local structure in a set fr√•om each node while respecting permutation equivariance, we construct a latent vector \\(\\mathbf{h}_i\\) from each \\(\\mathbf{x}_i\\) by applying the same learnable function \\(\\psi\\) to each node, and stack the results:\n\\[\n\\mathbf{h}_i = \\psi\\left(\\mathbf{x}_i\\right) ; ~~~ \\mathbf{H} = \\begin{bmatrix}\n           \\mathbf{h}_{1} \\\\\n           \\mathbf{h}_{2} \\\\\n           \\vdots \\\\\n           \\mathbf{h}_{n}\n         \\end{bmatrix} ~.\n\\]\nSet-level learning: We can generalise the above to sets by aggregating the latent vectors, which is permutation invariant with respect to the input nodes. Then (q: how much worse is it if we don‚Äôt?), we apply a second learnable function \\(\\phi\\) to arrive at the Deep Sets architecture:\n\\[\nf(\\mathbf{X}) = \\phi \\left(\\bigoplus_i \\psi \\left(\\mathbf{x}_i\\right)\\right)\n\\]\n\nIs Deep Sets the only way for learning on sets?\nFor many sets, apparently so! There are proofs referenced in the lectures that state that any learnable function that‚Äôs permutation invariant on sets can be reduced to the same expressivity as Deep Sets. Example: PointNet."
  },
  {
    "objectID": "sessions/notes/CS224W 2.1 ‚Äì Node-related graph features.html",
    "href": "sessions/notes/CS224W 2.1 ‚Äì Node-related graph features.html",
    "title": "A Living Book for Notes on Graph Neural Networks",
    "section": "",
    "text": "#cs224w #gnn-reading-group #graphic-content ### Preface for section 2 This section of the course discusses the pre-GNN paradigm. If we were looking to use a more off-the-shelf approach, like an MLP, we would need to find out how to best encode a graph as a set of input features. ## Node-level prediction The task of node-level prediction under this more ‚Äútraditional‚Äù paradigm reduces to the problem of finding \\[f(\\mathrm{node~\\nu}) \\rightarrow \\mathrm{result~about~node~\\nu}.\\] Here, \\(f\\) is likely going to be some learned function (e.g.¬†a feed-forward neural network), and knows nothing about the graph itself.\nTo make a statement about a node \\(\\nu\\), we begin by assuming that nodes already have their feature vectors associated with them, which we‚Äôve discussed previously (and used notation \\(\\mathbf{x}_\\nu\\) for). These will comprise some kind of domain information (e.g.¬†in a social network, each node would be a person, and carry relevant metadata for that person, like biometrics, date of joining‚Ä¶ stuff like this).\nAlong with these feature vectors, we now are required to encode the graph itself; there is no way for \\(f\\) to know about the graph structure unless we tell it otherwise! For each node, we will then need to include some other information about its relative importance in the graph structure, since we‚Äôve put the actual graph to one side for now. We‚Äôll have to craftily hand-design these features."
  },
  {
    "objectID": "sessions/notes/CS224W 2.1 ‚Äì Node-related graph features.html#graph-related-node-features",
    "href": "sessions/notes/CS224W 2.1 ‚Äì Node-related graph features.html#graph-related-node-features",
    "title": "A Living Book for Notes on Graph Neural Networks",
    "section": "Graph-related node features",
    "text": "Graph-related node features\nOur goal: come up with some numbers that characterize the structure and position of a particular node in the graph. Why is this useful? Consider the following picture:![[Pasted image 20240321130556.png]] Look at the graphs on the left and right. Can you tell the rule that makes the nodes colored red or green?\nThe answer is that red nodes have one connection, and green nodes have multiple. If the arrow labelled ‚Äúmachine learning‚Äù is to do its job correctly, it should definitely understand these notions of connectivity to exploit any geometric relations between nodes and edges.\n\nNode degree\nA first notion to look at here is node degree \\(k_\\nu\\), defined as the number of connections a node has. Here‚Äôs an illustration at nodes with different degrees: ![[Pasted image 20240321130940.png]] While this is information about the graph, it‚Äôs also misleading: we can have nodes with the same degree, but they might be very different in terms of their function in the graph! One could imagine a graph of organs in the body ‚Äî perhaps my foot and my heart have a similar number of veins in them, but I know which one I‚Äôd rather assign higher importance. We are then left wanting for some different ways to define node importance."
  },
  {
    "objectID": "sessions/notes/CS224W 2.1 ‚Äì Node-related graph features.html#node-centrality",
    "href": "sessions/notes/CS224W 2.1 ‚Äì Node-related graph features.html#node-centrality",
    "title": "A Living Book for Notes on Graph Neural Networks",
    "section": "Node centrality",
    "text": "Node centrality\nThere are a few measures of node centrality that we‚Äôll discuss, which is defined as a relative importance measure of a node, usually corresponding to some notion of how ‚Äúcentral‚Äù the node is in the network. Think of something like a key transit hub in a country, or the friend that‚Äôs the ‚Äúcenter‚Äù of a social network.\n\nBetweenness centrality\nFor a traffic graph, a node could be considered important if it‚Äôs a convenient way to get between many places quickly. For me, Liverpool Street is an example of this, since it always allows me to switch to the central/hammersmith & city/circle/metropolitan lines. Framing this using graph language would be to say that it lines on the shortest paths between many other node pairs. ![[Screenshot 2024-03-21 at 13.41.16.png]] ### Closeness centrality ![[Pasted image 20240321134318.png]]\n\n\nEigenvector centrality\nWhilst sounding a bit complicated, eigenvector centrality poses one thing: ‚ÄúI am as important as the sum of how important my friends are‚Äù. Let‚Äôs write that down in an equation, where we‚Äôll use \\(c_\\nu\\) as our measure of importance: \\[c_v = \\sum_{u \\in \\mathcal{N}(v)} c_u,\\]with notation of \\(\\mathcal{N}(v)\\) for the 1-hop neighborhood of \\(v\\). If we add some arbitrary postitive normalization \\(\\lambda\\) to this, we can actually take advantage of the adjacency matrix \\(\\mathbf{A}\\) to re-write this sum: \\[c_v = \\frac{1}{\\lambda}\\sum_{u \\neq v} A_{uv} c_u ~ \\Rightarrow ~ \\mathbf{Ac} = \\lambda\\mathbf{c}.\\] Casting things in this form allows us to extract a precise definition of \\(c_v\\) from taking the solutions to this eigenvalue equation. Specifically, due to the Perron-Frobenius theorem, the largest eigenvalue of this equation \\(\\lambda_{max}\\) is both positive and unique. It is the eigenvector associated with this (\\(\\mathbf{c}_{max}\\), to give it a name) that is used as the centrality measure."
  },
  {
    "objectID": "sessions/notes/CS224W 2.1 ‚Äì Node-related graph features.html#clustering-coefficient",
    "href": "sessions/notes/CS224W 2.1 ‚Äì Node-related graph features.html#clustering-coefficient",
    "title": "A Living Book for Notes on Graph Neural Networks",
    "section": "Clustering coefficient",
    "text": "Clustering coefficient\nHow connected are my friends? That‚Äôs another measure that could influence your relative importance in a graph. ![[Pasted image 20240321134456.png]] ## Graphlets A term called ego-network can be defined as the graph formed by a given node and its 1-hop neighbors. ![[Pasted image 20240321135014.png]] ![[Pasted image 20240321134838.png]] ![[Pasted image 20240321135138.png]]"
  },
  {
    "objectID": "sessions/notes/Geometric Deep Learning ‚Äì Graphs and Sets.html",
    "href": "sessions/notes/Geometric Deep Learning ‚Äì Graphs and Sets.html",
    "title": "First step: graphs without edges (sets)",
    "section": "",
    "text": "#gnn-reading-group #graphic-content ## Why are graphs/sets a useful blueprint for ‚Äúgeometric‚Äù deep learning?\nBoth a graph and a set provide a structure that is easy to analyze:"
  },
  {
    "objectID": "sessions/notes/Geometric Deep Learning ‚Äì Graphs and Sets.html#node-level-reasoning-and-equivariance",
    "href": "sessions/notes/Geometric Deep Learning ‚Äì Graphs and Sets.html#node-level-reasoning-and-equivariance",
    "title": "First step: graphs without edges (sets)",
    "section": "Node-level reasoning and equivariance",
    "text": "Node-level reasoning and equivariance\nYou‚Äôll notice that if we choose to aggregate over nodes, we‚Äôre actually losing information about each node individually in our output ‚Äî we can only make a statement about the set as a whole. In some applications, this may not be desirable behaviour; one could imagine wanting to make a classification statement about each node in your graph, or trying to predict a certain quantity on a per-node basis. How do we reconcile this with the notion of not caring about how the input is ordered?\nWe can zoom out a bit and recognise why we cared about permutation invariance at all: it was to ensure that the set was not accidentally treated as an ordered sequence, which may then propagate an ordering bias into the result of our calculations. However, perhaps there‚Äôs a notion of this that we‚Äôre happy with when predicting per-node quantities, since we‚Äôre more interested in the individual nodes than we are the whole set. If we are able to link each output to each node, then that should be enough to satisfy us, provided that this holds no matter how we shuffle the input.\nTo formalise this: For any permutation matrix \\(\\mathbf{P}\\), we want to be able to apply \\(\\mathbf{P}\\) to the input, and still be able to link each output to the right node, i.e.¬†\\(f(\\mathbf{X})\\) should also change in the same way. For a general operation, this property is called equivariance: the output changes in the same way as the input if we apply an operation to just the input. Equivalently, we could say that if we applied the operation to the output, the result will be as if we did so for the input. For the case of permutations, we can write the following:\nPermutation equivariance of \\(f\\): For any permutation matrix \\(\\mathbf{P}\\), we require \\(f(\\mathbf{P}\\mathbf{X}) = \\mathbf{P}f(\\mathbf{X})\\). The output changes in the same way as the input.\n\nAside: Locality as a constraint\nImagine wanting to predict a label on an image, but wanting to stay robust to translations of that image. We could either force the network to learn this property by adding translation as a data augmentation, or we could build it directly into the architecture somehow, e.g.¬†by pooling operations in CNNs (which are equivariant to spacial translations ‚Äî the pooled values move in-step with the image).\nIn practice, it‚Äôs highly likely that a pure translation of the image is not the only thing to worry about. As an example, imagine taking a picture of a house from two different angles. Now you suddenly have a slightly different shape, and maybe a bird is on the roof in the second picture! We‚Äôd like to be robust, then, to not just shifts, but also to any deformations of the input that come along for the ride (see image from slides below).\n\n\n\nUntitled\n\n\nPossible solution: compose many small local operations, but do this very deep (e.g.¬†small kernels in CNNs, but many layers). Local operations should not propagate any errors to the global picture. (?)\n\n\nHow do we enforce locality for equivariant functions on sets?\nAn easy way to retain equivariance (one node input links to one output) and locality (learning happens on a small set of nodes) is to just operate on each node individually ‚Äî that is, we apply the same function \\(\\psi\\) to each element separately, and get a set of latents \\(\\mathbf{h}_i\\).\n\n\n\nUntitled\n\n\nThis might sound familiar ‚Äî it‚Äôs the inner part of the Deep Sets architecture (before aggregating with the sum)."
  },
  {
    "objectID": "sessions/notes/Geometric Deep Learning ‚Äì Graphs and Sets.html#learning-on-sets-summary",
    "href": "sessions/notes/Geometric Deep Learning ‚Äì Graphs and Sets.html#learning-on-sets-summary",
    "title": "First step: graphs without edges (sets)",
    "section": "Learning on sets: summary",
    "text": "Learning on sets: summary\nRecall that sets are, by definition, an unordered set of objects, and we‚Äôre operating in a way that preserves this behaviour.\nNode-level learning: To learn local structure in a set fr√•om each node while respecting permutation equivariance, we construct a latent vector \\(\\mathbf{h}_i\\) from each \\(\\mathbf{x}_i\\) by applying the same learnable function \\(\\psi\\) to each node, and stack the results:\n\\[\n\\mathbf{h}_i = \\psi\\left(\\mathbf{x}_i\\right) ; ~~~ \\mathbf{H} = \\begin{bmatrix}\n           \\mathbf{h}_{1} \\\\\n           \\mathbf{h}_{2} \\\\\n           \\vdots \\\\\n           \\mathbf{h}_{n}\n         \\end{bmatrix} ~.\n\\]\nSet-level learning: We can generalise the above to sets by aggregating the latent vectors, which is permutation invariant with respect to the input nodes. Then (q: how much worse is it if we don‚Äôt?), we apply a second learnable function \\(\\phi\\) to arrive at the Deep Sets architecture:\n\\[\nf(\\mathbf{X}) = \\phi \\left(\\bigoplus_i \\psi \\left(\\mathbf{x}_i\\right)\\right)\n\\]\n\nIs Deep Sets the only way for learning on sets?\nFor many sets, apparently so! There are proofs referenced in the lectures that state that any learnable function that‚Äôs permutation invariant on sets can be reduced to the same expressivity as Deep Sets. Example: PointNet."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nThis is a living book of notes for the graph neural network reading group at The Alan Turing Institute.\nTo contribute, please make a PR to the repository linked above after we‚Äôve had a session on your chosen topic. Write your notes in any of the supported formats for Quarto (.qmd, .md, or .ipynb), and Quarto will do all of the legwork to publish it to this webpage!"
  }
]