<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Graphs and Sets (Geometric Deep Learning) ‚Äì A Living Book for Notes on Graph Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">A Living Book for Notes on Graph Neural Networks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../sessions.html"> 
<span class="menu-text">Sessions</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Graphs and Sets (Geometric Deep Learning)</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="why-are-graphssets-a-useful-blueprint-for-geometric-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="why-are-graphssets-a-useful-blueprint-for-geometric-deep-learning">Why are graphs/sets a useful blueprint for ‚Äúgeometric‚Äù deep learning?</h2>
<p>Both a graph and a set provide a structure that is easy to analyze:</p>
<ul>
<li>Domain is <strong>discrete</strong> (nodes or nodes + edges)</li>
<li>Minimal geometric assumptions [there was a comment on ‚Äúresistance to permutations‚Äù that I didn‚Äôt get]</li>
</ul>
<section id="what-kind-of-data-is-graph-like" class="level3">
<h3 class="anchored" data-anchor-id="what-kind-of-data-is-graph-like">What kind of data is graph-like?</h3>
<p>Basically anything! Fun example: tube map üòä</p>
<p>Also google maps: optimal route from A ‚Äî&gt; B probably goes through a graph neural network.</p>
</section>
</section>
<section id="first-step-graphs-without-edges-sets" class="level2">
<h2 class="anchored" data-anchor-id="first-step-graphs-without-edges-sets">First step: graphs without edges (sets)</h2>
<p>j Useful for point-cloud like structures, unordered collections of objects. We can begin by defining a set of <span class="math inline">\(n\)</span> nodes, each with a feature vector <span class="math inline">\(\mathbf{x}_i\)</span> of length <span class="math inline">\(v\)</span>, giving us the <span class="math inline">\(n \times v\)</span> feature matrix <span class="math inline">\(\mathbf{X}\)</span>, where every row is a set of features for one node:</p>
<p><span class="math display">\[
\mathbf{X} = \begin{bmatrix}
           \mathbf{x}_{1} \\
           \mathbf{x}_{2} \\
           \vdots \\
           \mathbf{x}_{n}
         \end{bmatrix} ~.
\]</span></p>
<p>Ah, but wait: I‚Äôve chosen to number the nodes 1 through <span class="math inline">\(n\)</span> right? That means I‚Äôve defined an ordering! We need to make sure that the result of any calculation involving <span class="math inline">\(\mathbf{}\)</span><span class="math inline">\(\mathbf{X}\)</span> is <strong>invariant</strong> to the ordering of the nodes, since we want to treat this as an unordered collection.</p>
<p>Another way of putting this is that we want the result of applying our calculation to be equal for all possible orderings of <span class="math inline">\(\mathbf{X}\)</span>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/graphs-and-sets/Untitled.png" class="img-fluid figure-img"></p>
<figcaption>Untitled</figcaption>
</figure>
</div>
</section>
<section id="why-are-graphssets-a-useful-blueprint-for-geometric-deep-learning-1" class="level2">
<h2 class="anchored" data-anchor-id="why-are-graphssets-a-useful-blueprint-for-geometric-deep-learning-1">Why are graphs/sets a useful blueprint for ‚Äúgeometric‚Äù deep learning?</h2>
<p>Both a graph and a set provide a structure that is easy to analyze:</p>
<ul>
<li>Domain is <strong>discrete</strong> (nodes or nodes + edges)</li>
<li>Minimal geometric assumptions [there was a comment on ‚Äúresistance to permutations‚Äù that I didn‚Äôt get]</li>
</ul>
<section id="what-kind-of-data-is-graph-like-1" class="level3">
<h3 class="anchored" data-anchor-id="what-kind-of-data-is-graph-like-1">What kind of data is graph-like?</h3>
<p>Basically anything! Fun example: tube map üòä</p>
<p>Also google maps: optimal route from A ‚Äî&gt; B probably goes through a graph neural network.</p>
</section>
</section>
<section id="first-step-graphs-without-edges-sets-1" class="level2">
<h2 class="anchored" data-anchor-id="first-step-graphs-without-edges-sets-1">First step: graphs without edges (sets)</h2>
<p>j Useful for point-cloud like structures, unordered collections of objects. We can begin by defining a set of <span class="math inline">\(n\)</span> nodes, each with a feature vector <span class="math inline">\(\mathbf{x}_i\)</span> of length <span class="math inline">\(v\)</span>, giving us the <span class="math inline">\(n \times v\)</span> feature matrix <span class="math inline">\(\mathbf{X}\)</span>, where every row is a set of features for one node:</p>
<p><span class="math display">\[
\mathbf{X} = \begin{bmatrix}
           \mathbf{x}_{1} \\
           \mathbf{x}_{2} \\
           \vdots \\
           \mathbf{x}_{n}
         \end{bmatrix} ~.
\]</span></p>
<p>Ah, but wait: I‚Äôve chosen to number the nodes 1 through <span class="math inline">\(n\)</span> right? That means I‚Äôve defined an ordering! We need to make sure that the result of any calculation involving <span class="math inline">\(\mathbf{}\)</span><span class="math inline">\(\mathbf{X}\)</span> is <strong>invariant</strong> to the ordering of the nodes, since we want to treat this as an unordered collection.</p>
<p>Another way of putting this is that we want the result of applying our calculation to be equal for all possible orderings of <span class="math inline">\(\mathbf{X}\)</span>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/graphs-and-sets/Untitled.png" class="img-fluid figure-img"></p>
<figcaption>Untitled</figcaption>
</figure>
</div>
<p>This is a function <span class="math inline">\(f\)</span> applied to two different orderings, or <strong>permutations</strong> of nodes, and we‚Äôre requiring that they‚Äôre equal in output. To permute the nodes is to change their order; we want this condition of equality to hold over all possible permutations.</p>
<p>We can write permutations of <span class="math inline">\(\mathbf{X}\)</span> as the product of <span class="math inline">\(\mathbf{X}\)</span> with some <em>permutation matrix <span class="math inline">\(\mathbf{P}\)</span></em>, which is square in the feature dimension <span class="math inline">\(v\)</span>, and has exactly one 1 in each row and column. The only result of applying <span class="math inline">\(\mathbf{P}\)</span> is the reordering of the feature vector order. Example:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/graphs-and-sets/Untitled 1.png" class="img-fluid figure-img"></p>
<figcaption>Untitled</figcaption>
</figure>
</div>
<p>Given this, we can summarise permutation invariance through the following equation:</p>
<p><strong>Permutation invariance of <span class="math inline">\(f\)</span>:</strong> For any permutation matrix <span class="math inline">\(\mathbf{P}\)</span>, we require <span class="math inline">\(f(\mathbf{P}\mathbf{X}) = f(\mathbf{X})\)</span>. The output is <em>unaffected by re-ordering the input</em>.</p>
<section id="example-deep-sets" class="level3">
<h3 class="anchored" data-anchor-id="example-deep-sets">Example: Deep Sets</h3>
<p>Given two learnable functions <span class="math inline">\(\phi,\psi\)</span>, the <strong>deep sets</strong> architecture ‚Äî proposed initially in 2018 ‚Äî has the following choice of <span class="math inline">\(f\)</span>:</p>
<p><span class="math display">\[
f(\mathbf{X}) = \phi\left(\sum_{i=1}^{v}\psi(\mathbf{x}_i)\right),
\]</span></p>
<p>where summing the learned functions for each node enforces the permutation invariance property (it doesn‚Äôt matter what order you sum ‚Äî you always get the same result). A concrete example of this would be to choose both <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\psi\)</span> to be feed-forward neural networks (also called MLPs/multi-layer perceptrons).</p>
<p>Note that summing here is just one choice of information aggregation ‚Äî we could have just as easily chosen the maximum, empirical mean etc. The point is that all of these operations are <em>invariant to the order of the input</em>. The optimal way to aggregate information is a design choice, and may vary depending on the problem you‚Äôre solving and the properties you want to learn.</p>
<p>In future, we‚Äôll use <span class="math inline">\(\bigoplus\)</span> to denote a general aggregator function, which needs to be chosen when actually implementing the architecture involving it.</p>
</section>
</section>
<section id="node-level-reasoning-and-equivariance" class="level2">
<h2 class="anchored" data-anchor-id="node-level-reasoning-and-equivariance">Node-level reasoning and equivariance</h2>
<p>You‚Äôll notice that if we choose to aggregate over nodes, we‚Äôre actually losing information about each node individually in our output ‚Äî we can only make a statement about the set as a whole. In some applications, this may not be desirable behaviour; one could imagine wanting to make a classification statement about each node in your graph, or trying to predict a certain quantity on a per-node basis. How do we reconcile this with the notion of not caring about how the input is ordered?</p>
<p>We can zoom out a bit and recognise why we cared about permutation invariance at all: it was to ensure that the set was not accidentally treated as an ordered sequence, which may then propagate an ordering bias into the result of our calculations. However, perhaps there‚Äôs a notion of this that we‚Äôre happy with when predicting per-node quantities, since we‚Äôre more interested in the individual nodes than we are the whole set. If we are able to link each output to each node, then that should be enough to satisfy us, provided that this holds no matter how we shuffle the input.</p>
<p>To formalise this: For any permutation matrix <span class="math inline">\(\mathbf{P}\)</span>, we want to be able to apply <span class="math inline">\(\mathbf{P}\)</span> to the input, and still be able to link each output to the right node, i.e.&nbsp;<span class="math inline">\(f(\mathbf{X})\)</span> should also change in the same way. For a general operation, this property is called <strong>equivariance</strong>: the output changes <em>in the same way</em> as the input if we apply an operation to <em>just</em> <em>the input</em>. Equivalently, we could say that if we applied the operation to the <em>output</em>, the result will be as if we did so for the <em>input</em>. For the case of permutations, we can write the following:</p>
<p><strong>Permutation equivariance of <span class="math inline">\(f\)</span>:</strong> For any permutation matrix <span class="math inline">\(\mathbf{P}\)</span>, we require <span class="math inline">\(f(\mathbf{P}\mathbf{X}) = \mathbf{P}f(\mathbf{X})\)</span>. The output changes <em>in the same way</em> <em>as the input</em>.</p>
<section id="aside-locality-as-a-constraint" class="level3">
<h3 class="anchored" data-anchor-id="aside-locality-as-a-constraint">Aside: Locality as a constraint</h3>
<p>Imagine wanting to predict a label on an image, but wanting to stay robust to translations of that image. We could either force the network to learn this property by adding translation as a data augmentation, or we could build it directly into the architecture somehow, e.g.&nbsp;by pooling operations in CNNs (which are <em>equivariant</em> to spacial translations ‚Äî the pooled values move in-step with the image).</p>
<p>In practice, it‚Äôs highly likely that a pure translation of the image is not the only thing to worry about. As an example, imagine taking a picture of a house from two different angles. Now you suddenly have a slightly different shape, and maybe a bird is on the roof in the second picture! We‚Äôd like to be robust, then, to not just shifts, but also to any deformations of the input that come along for the ride (see image from slides below).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/graphs-and-sets/Untitled 2.png" class="img-fluid figure-img"></p>
<figcaption>Untitled</figcaption>
</figure>
</div>
<p>Possible solution: compose many small local operations, but do this very deep (e.g.&nbsp;small kernels in CNNs, but many layers). Local operations should not propagate any errors to the global picture. (?)</p>
</section>
<section id="how-do-we-enforce-locality-for-equivariant-functions-on-sets" class="level3">
<h3 class="anchored" data-anchor-id="how-do-we-enforce-locality-for-equivariant-functions-on-sets">How do we enforce locality for equivariant functions on sets?</h3>
<p>An easy way to retain equivariance (one node input links to one output) and locality (learning happens on a small set of nodes) is to just operate on each node individually ‚Äî that is, we apply the same function <span class="math inline">\(\psi\)</span> to each element separately, and get a set of latents <span class="math inline">\(\mathbf{h}_i\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/graphs-and-sets/Untitled 3.png" class="img-fluid figure-img"></p>
<figcaption>Untitled</figcaption>
</figure>
</div>
<p>This might sound familiar ‚Äî it‚Äôs the inner part of the Deep Sets architecture (before aggregating with the sum).</p>
</section>
</section>
<section id="learning-on-sets-summary" class="level2">
<h2 class="anchored" data-anchor-id="learning-on-sets-summary">Learning on sets: summary</h2>
<p>Recall that sets are, by definition, an <strong>unordered set of objects</strong>, and we‚Äôre operating in a way that preserves this behaviour.</p>
<p><strong>Node-level learning:</strong> To learn <strong><em>local structure</em></strong> in a set fr√•om each node while respecting <strong><em>permutation equivariance</em></strong>, we construct a latent vector <span class="math inline">\(\mathbf{h}_i\)</span> from each <span class="math inline">\(\mathbf{x}_i\)</span> by applying the same learnable function <span class="math inline">\(\psi\)</span> to each node, and stack the results:</p>
<p><span class="math display">\[
\mathbf{h}_i = \psi\left(\mathbf{x}_i\right) ; ~~~ \mathbf{H} = \begin{bmatrix}
           \mathbf{h}_{1} \\
           \mathbf{h}_{2} \\
           \vdots \\
           \mathbf{h}_{n}
         \end{bmatrix} ~.
\]</span></p>
<p><strong>Set-level learning:</strong> We can generalise the above to sets by <em>aggregating the latent vectors</em>, which is <strong><em>permutation invariant with respect to the input nodes</em></strong>. Then (q: how much worse is it if we don‚Äôt?), we apply a second learnable function <span class="math inline">\(\phi\)</span> to arrive at the Deep Sets architecture:</p>
<p><span class="math display">\[
f(\mathbf{X}) = \phi \left(\bigoplus_i \psi \left(\mathbf{x}_i\right)\right)
\]</span></p>
<section id="is-deep-sets-the-only-way-for-learning-on-sets" class="level3">
<h3 class="anchored" data-anchor-id="is-deep-sets-the-only-way-for-learning-on-sets">Is Deep Sets the only way for learning on sets?</h3>
<p>For many sets, apparently so! There are proofs referenced in the lectures that state that <em>any</em> learnable function that‚Äôs permutation invariant on sets can be reduced to the same expressivity as Deep Sets. Example: PointNet.</p>
<p>This is a function <span class="math inline">\(f\)</span> applied to two different orderings, or <strong>permutations</strong> of nodes, and we‚Äôre requiring that they‚Äôre equal in output. To permute the nodes is to change their order; we want this condition of equality to hold over all possible permutations.</p>
<p>We can write permutations of <span class="math inline">\(\mathbf{X}\)</span> as the product of <span class="math inline">\(\mathbf{X}\)</span> with some <em>permutation matrix <span class="math inline">\(\mathbf{P}\)</span></em>, which is square in the feature dimension <span class="math inline">\(v\)</span>, and has exactly one 1 in each row and column. The only result of applying <span class="math inline">\(\mathbf{P}\)</span> is the reordering of the feature vector order. Example:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/graphs-and-sets/Untitled 1.png" class="img-fluid figure-img"></p>
<figcaption>Untitled</figcaption>
</figure>
</div>
<p>Given this, we can summarise permutation invariance through the following equation:</p>
<p><strong>Permutation invariance of <span class="math inline">\(f\)</span>:</strong> For any permutation matrix <span class="math inline">\(\mathbf{P}\)</span>, we require <span class="math inline">\(f(\mathbf{P}\mathbf{X}) = f(\mathbf{X})\)</span>. The output is <em>unaffected by re-ordering the input</em>.</p>
</section>
<section id="example-deep-sets-1" class="level3">
<h3 class="anchored" data-anchor-id="example-deep-sets-1">Example: Deep Sets</h3>
<p>Given two learnable functions <span class="math inline">\(\phi,\psi\)</span>, the <strong>deep sets</strong> architecture ‚Äî proposed initially in 2018 ‚Äî has the following choice of <span class="math inline">\(f\)</span>:</p>
<p><span class="math display">\[
f(\mathbf{X}) = \phi\left(\sum_{i=1}^{v}\psi(\mathbf{x}_i)\right),
\]</span></p>
<p>where summing the learned functions for each node enforces the permutation invariance property (it doesn‚Äôt matter what order you sum ‚Äî you always get the same result). A concrete example of this would be to choose both <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\psi\)</span> to be feed-forward neural networks (also called MLPs/multi-layer perceptrons).</p>
<p>Note that summing here is just one choice of information aggregation ‚Äî we could have just as easily chosen the maximum, empirical mean etc. The point is that all of these operations are <em>invariant to the order of the input</em>. The optimal way to aggregate information is a design choice, and may vary depending on the problem you‚Äôre solving and the properties you want to learn.</p>
<p>In future, we‚Äôll use <span class="math inline">\(\bigoplus\)</span> to denote a general aggregator function, which needs to be chosen when actually implementing the architecture involving it.</p>
</section>
</section>
<section id="node-level-reasoning-and-equivariance-1" class="level2">
<h2 class="anchored" data-anchor-id="node-level-reasoning-and-equivariance-1">Node-level reasoning and equivariance</h2>
<p>You‚Äôll notice that if we choose to aggregate over nodes, we‚Äôre actually losing information about each node individually in our output ‚Äî we can only make a statement about the set as a whole. In some applications, this may not be desirable behaviour; one could imagine wanting to make a classification statement about each node in your graph, or trying to predict a certain quantity on a per-node basis. How do we reconcile this with the notion of not caring about how the input is ordered?</p>
<p>We can zoom out a bit and recognise why we cared about permutation invariance at all: it was to ensure that the set was not accidentally treated as an ordered sequence, which may then propagate an ordering bias into the result of our calculations. However, perhaps there‚Äôs a notion of this that we‚Äôre happy with when predicting per-node quantities, since we‚Äôre more interested in the individual nodes than we are the whole set. If we are able to link each output to each node, then that should be enough to satisfy us, provided that this holds no matter how we shuffle the input.</p>
<p>To formalise this: For any permutation matrix <span class="math inline">\(\mathbf{P}\)</span>, we want to be able to apply <span class="math inline">\(\mathbf{P}\)</span> to the input, and still be able to link each output to the right node, i.e.&nbsp;<span class="math inline">\(f(\mathbf{X})\)</span> should also change in the same way. For a general operation, this property is called <strong>equivariance</strong>: the output changes <em>in the same way</em> as the input if we apply an operation to <em>just</em> <em>the input</em>. Equivalently, we could say that if we applied the operation to the <em>output</em>, the result will be as if we did so for the <em>input</em>. For the case of permutations, we can write the following:</p>
<p><strong>Permutation equivariance of <span class="math inline">\(f\)</span>:</strong> For any permutation matrix <span class="math inline">\(\mathbf{P}\)</span>, we require <span class="math inline">\(f(\mathbf{P}\mathbf{X}) = \mathbf{P}f(\mathbf{X})\)</span>. The output changes <em>in the same way</em> <em>as the input</em>.</p>
<section id="aside-locality-as-a-constraint-1" class="level3">
<h3 class="anchored" data-anchor-id="aside-locality-as-a-constraint-1">Aside: Locality as a constraint</h3>
<p>Imagine wanting to predict a label on an image, but wanting to stay robust to translations of that image. We could either force the network to learn this property by adding translation as a data augmentation, or we could build it directly into the architecture somehow, e.g.&nbsp;by pooling operations in CNNs (which are <em>equivariant</em> to spacial translations ‚Äî the pooled values move in-step with the image).</p>
<p>In practice, it‚Äôs highly likely that a pure translation of the image is not the only thing to worry about. As an example, imagine taking a picture of a house from two different angles. Now you suddenly have a slightly different shape, and maybe a bird is on the roof in the second picture! We‚Äôd like to be robust, then, to not just shifts, but also to any deformations of the input that come along for the ride (see image from slides below).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/graphs-and-sets/Untitled 2.png" class="img-fluid figure-img"></p>
<figcaption>Untitled</figcaption>
</figure>
</div>
<p>Possible solution: compose many small local operations, but do this very deep (e.g.&nbsp;small kernels in CNNs, but many layers). Local operations should not propagate any errors to the global picture. (?)</p>
</section>
<section id="how-do-we-enforce-locality-for-equivariant-functions-on-sets-1" class="level3">
<h3 class="anchored" data-anchor-id="how-do-we-enforce-locality-for-equivariant-functions-on-sets-1">How do we enforce locality for equivariant functions on sets?</h3>
<p>An easy way to retain equivariance (one node input links to one output) and locality (learning happens on a small set of nodes) is to just operate on each node individually ‚Äî that is, we apply the same function <span class="math inline">\(\psi\)</span> to each element separately, and get a set of latents <span class="math inline">\(\mathbf{h}_i\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/graphs-and-sets/Untitled 3.png" class="img-fluid figure-img"></p>
<figcaption>Untitled</figcaption>
</figure>
</div>
<p>This might sound familiar ‚Äî it‚Äôs the inner part of the Deep Sets architecture (before aggregating with the sum).</p>
</section>
</section>
<section id="learning-on-sets-summary-1" class="level2">
<h2 class="anchored" data-anchor-id="learning-on-sets-summary-1">Learning on sets: summary</h2>
<p>Recall that sets are, by definition, an <strong>unordered set of objects</strong>, and we‚Äôre operating in a way that preserves this behaviour.</p>
<p><strong>Node-level learning:</strong> To learn <strong><em>local structure</em></strong> in a set fr√•om each node while respecting <strong><em>permutation equivariance</em></strong>, we construct a latent vector <span class="math inline">\(\mathbf{h}_i\)</span> from each <span class="math inline">\(\mathbf{x}_i\)</span> by applying the same learnable function <span class="math inline">\(\psi\)</span> to each node, and stack the results:</p>
<p><span class="math display">\[
\mathbf{h}_i = \psi\left(\mathbf{x}_i\right) ; ~~~ \mathbf{H} = \begin{bmatrix}
           \mathbf{h}_{1} \\
           \mathbf{h}_{2} \\
           \vdots \\
           \mathbf{h}_{n}
         \end{bmatrix} ~.
\]</span></p>
<p><strong>Set-level learning:</strong> We can generalise the above to sets by <em>aggregating the latent vectors</em>, which is <strong><em>permutation invariant with respect to the input nodes</em></strong>. Then (q: how much worse is it if we don‚Äôt?), we apply a second learnable function <span class="math inline">\(\phi\)</span> to arrive at the Deep Sets architecture:</p>
<p><span class="math display">\[
f(\mathbf{X}) = \phi \left(\bigoplus_i \psi \left(\mathbf{x}_i\right)\right)
\]</span></p>
<section id="is-deep-sets-the-only-way-for-learning-on-sets-1" class="level3">
<h3 class="anchored" data-anchor-id="is-deep-sets-the-only-way-for-learning-on-sets-1">Is Deep Sets the only way for learning on sets?</h3>
<p>For many sets, apparently so! There are proofs referenced in the lectures that state that <em>any</em> learnable function that‚Äôs permutation invariant on sets can be reduced to the same expressivity as Deep Sets. Example: PointNet.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/alan-turing-institute\.github\.io\/gnn-reading-group\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>